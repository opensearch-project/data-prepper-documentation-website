{"0": {
    "doc": "add_entries",
    "title": "add_entries",
    "content": "# add_entries The `add_entries` processor adds entries to an event. ### Configuration You can configure the `add_entries` processor with the following options. | Option | Required | Description | :--- | :--- | :--- | `entries` | Yes | A list of entries to add to an event. | `key` | Yes | The key of the new entry to be added. Some examples of keys include `my_key`, `myKey`, and `object/sub_Key`. | `value` | Yes | The value of the new entry to be added. You can use the following data types: strings, Booleans, numbers, null, nested objects, and arrays. | `overwrite_if_key_exists` | No | When set to `true`, the existing value is overwritten if `key` already exists in the event. The default value is `false`. | ### Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: ...... processor: - add_entries: entries: - key: \"newMessage\" value: 3 overwrite_if_key_exists: true sink: ``` {% include copy.html %} For example, when your source contains the following event record: ```json {\"message\": \"hello\"} ``` And then you run the `add_entries` processor using the example pipeline, it adds a new entry, `{\"newMessage\": 3}`, to the existing event, `{\"message\": \"hello\"}`, so that the new event contains two entries in the final output: ```json {\"message\": \"hello\", \"newMessage\": 3} ``` > If `newMessage` already exists, its existing value is overwritten with a value of `3`. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/add-entries/",
    "relUrl": "/pipelines/configuration/processors/add-entries/"
  },"1": {
    "doc": "aggregate",
    "title": "aggregate",
    "content": "# aggregate The `aggregate` processor groups events based on the values of `identification_keys`. Then, the processor performs an action on each group, helping reduce unnecessary log volume and creating aggregated logs over time. You can use existing actions or create your own custom aggregations using Java code. ## Configuration The following table describes the options you can use to configure the `aggregate` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- identification_keys | Yes | List | An unordered list by which to group events. Events with the same values as these keys are put into the same group. If an event does not contain one of the `identification_keys`, then the value of that key is considered to be equal to `null`. At least one identification_key is required (for example, `[\"sourceIp\", \"destinationIp\", \"port\"]`). action | Yes | AggregateAction | The action to be performed on each group. One of the [available aggregate actions](#available-aggregate-actions) must be provided, or you can create custom aggregate actions. `remove_duplicates` and `put_all` are the available actions. For more information, see [Creating New Aggregate Actions](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/aggregate-processor#creating-new-aggregate-actions). group_duration | No | String | The amount of time that a group should exist before it is concluded automatically. Supports ISO_8601 notation strings (\"PT20.345S\", \"PT15M\", etc.) as well as simple notation for seconds (`\"60s\"`) and milliseconds (`\"1500ms\"`). Default value is `180s`. ## Available aggregate actions Use the following aggregate actions to determine how the `aggregate` processor processes events in each group. ### remove_duplicates The `remove_duplicates` action processes the first event for a group immediately and drops any events that duplicate the first event from the source. For example, when using `identification_keys: [\"sourceIp\", \"destination_ip\"]`: 1. The `remove_duplicates` action processes `{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 }`, the first event in the source. 2. Data Prepper drops the `{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 }` event because the `sourceIp` and `destinationIp` match the first event in the source. 3. The `remove_duplicates` action processes the next event, `{ \"sourceIp\": \"127.0.0.2\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 }`. Because the `sourceIp` is different from the first event of the group, Data Prepper creates a new group based on the event. ### put_all The `put_all` action combines events belonging to the same group by overwriting existing keys and adding new keys, similarly to the Java `Map.putAll`. The action drops all events that make up the combined event. For example, when using `identification_keys: [\"sourceIp\", \"destination_ip\"]`, the `put_all` action processes the following three events: ``` { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"http_verb\": \"GET\" } ``` Then the action combines the events into one. The pipeline then uses the following combined event: ``` { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200, \"bytes\": 1000, \"http_verb\": \"GET\" } ``` ### count The `count` event counts events that belong to the same group and generates a new event with values of the `identification_keys` and the count, which indicates the number of new events. You can customize the processor with the following configuration options: * `count_key`: Key used for storing the count. Default name is `aggr._count`. * `start_time_key`: Key used for storing the start time. Default name is `aggr._start_time`. * `output_format`: Format of the aggregated event. * `otel_metrics`: Default output format. Outputs in OTel metrics SUM type with count as value. * `raw` - Generates a JSON object with the `count_key` field as a count value and the `start_time_key` field with aggregation start time as value. For an example, when using `identification_keys: [\"sourceIp\", \"destination_ip\"]`, the `count` action counts and processes the following events: ```json { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 503 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 400 } ``` The processor creates the following event: ```json {\"isMonotonic\":true,\"unit\":\"1\",\"aggregationTemporality\":\"AGGREGATION_TEMPORALITY_DELTA\",\"kind\":\"SUM\",\"name\":\"count\",\"description\":\"Number of events\",\"startTime\":\"2022-12-02T19:29:51.245358486Z\",\"time\":\"2022-12-02T19:30:15.247799684Z\",\"value\":3.0,\"sourceIp\":\"127.0.0.1\",\"destinationIp\":\"192.168.0.1\"} ``` ### histogram The `histogram` action aggregates events belonging to the same group and generates a new event with values of the `identification_keys` and histogram of the aggregated events based on a configured `key`. The histogram contains the number of events, sum, buckets, bucket counts, and optionally min and max of the values corresponding to the `key`. The action drops all events that make up the combined event. You can customize the processor with the following configuration options: * `key`: Name of the field in the events the histogram generates. * `generated_key_prefix`: `key_prefix` used by all the fields created in the aggregated event. Having a prefix ensures that the names of the histogram event do not conflict with the field names in the event. * `units`: The units for the values in the `key`. * `record_minmax`: A Boolean value indicating whether the histogram should include the min and max of the values in the aggregation. * `buckets`: A list of buckets (values of type `double`) indicating the buckets in the histogram. * `output_format`: Format of the aggregated event. * `otel_metrics`: Default output format. Outputs in OTel metrics SUM type with count as value. * `raw`: Generates a JSON object with `count_key` field with count as value and `start_time_key` field with aggregation start time as value. For example, when using `identification_keys: [\"sourceIp\", \"destination_ip\", \"request\"]`, `key: latency`, and `buckets: [0.0, 0.25, 0.5]`, the `histogram` action processes the following events: ``` { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\" : \"/index.html\", \"latency\": 0.2 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\" : \"/index.html\", \"latency\": 0.55} { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\" : \"/index.html\", \"latency\": 0.25 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\" : \"/index.html\", \"latency\": 0.15 } ``` Then the processor creates the following event: ```json {\"max\":0.55,\"kind\":\"HISTOGRAM\",\"buckets\":[{\"min\":-3.4028234663852886E38,\"max\":0.0,\"count\":0},{\"min\":0.0,\"max\":0.25,\"count\":2},{\"min\":0.25,\"max\":0.50,\"count\":1},{\"min\":0.50,\"max\":3.4028234663852886E38,\"count\":1}],\"count\":4,\"bucketCountsList\":[0,2,1,1],\"description\":\"Histogram of latency in the events\",\"sum\":1.15,\"unit\":\"seconds\",\"aggregationTemporality\":\"AGGREGATION_TEMPORALITY_DELTA\",\"min\":0.15,\"bucketCounts\":4,\"name\":\"histogram\",\"startTime\":\"2022-12-14T06:43:40.848762215Z\",\"explicitBoundsCount\":3,\"time\":\"2022-12-14T06:44:04.852564623Z\",\"explicitBounds\":[0.0,0.25,0.5],\"request\":\"/index.html\",\"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"key\": \"latency\"} ``` ### rate_limiter The `rate_limiter` action controls the number of events aggregated per second. By default, `rate_limiter` blocks the `aggregate` processor from running if it receives more events than the configured number allowed. You can overwrite the number events that triggers the `rate_limited` by using the `when_exceeds` configuration option. You can customize the processor with the following configuration options: * `events_per_second`: The number of events allowed per second. * `when_exceeds`: Indicates what action the `rate_limiter` takes when the number of events received is greater than the number of events allowed per second. Default value is `block`, which blocks the processor from running after the maximum number of events allowed per second is reached until the next second. Alternatively, the `drop` option drops the excess events received in that second. For example, if `events_per_second` is set to `1` and `when_exceeds` is set to `drop`, the action tries to process the following events when received during the one second time interval: ```json { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"http_verb\": \"GET\" } ``` The following event is processed, but all other events are ignored because the `rate_limiter` blocks them: ```json { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } ``` If `when_exceeds` is set to `drop`, all three events are processed. ### percent_sampler The `percent_sampler` action controls the number of events aggregated based on a percentage of events. The action drops any events not included in the percentage. You can set the percentage of events using the `percent` configuration, which indicates the percentage of events processed during a one second interval (0%--100%). For example, if percent is set to `50`, the action tries to process the following events in the one-second interval: ``` { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 2500 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 500 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 3100 } ``` The pipeline processes 50% of the events, drops the other events, and does not generate a new event: ``` { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 500 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 3100 } ``` ## Metrics The following table describes common [Abstract processor](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/processor/AbstractProcessor.java) metrics. | Metric name | Type | Description | ------------- | ---- | -----------| `recordsIn` | Counter | Metric representing the ingress of records to a pipeline component. | `recordsOut` | Counter | Metric representing the egress of records from a pipeline component. | `timeElapsed` | Timer | Metric representing the time elapsed during execution of a pipeline component. | The `aggregate` processor includes the following custom metrics. **Counter** * `actionHandleEventsOut`: The number of events that have been returned from the `handleEvent` call to the configured [action](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/aggregate-processor#action). * `actionHandleEventsDropped`: The number of events that have not been returned from the `handleEvent` call to the configured [action](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/aggregate-processor#action). * `actionHandleEventsProcessingErrors`: The number of calls made to `handleEvent` for the configured [action](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/aggregate-processor#action) that resulted in an error. * `actionConcludeGroupEventsOut`: The number of events that have been returned from the `concludeGroup` call to the configured [action](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/aggregate-processor#action). * `actionConcludeGroupEventsDropped`: The number of events that have not been returned from the `condludeGroup` call to the configured [action](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/aggregate-processor#action). * `actionConcludeGroupEventsProcessingErrors`: The number of calls made to `concludeGroup` for the configured [action](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/aggregate-processor#action) that resulted in an error. **Gauge** * `currentAggregateGroups`: The current number of groups. This gauge decreases when a group concludes and increases when an event initiates the creation of a new group. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/aggregate/",
    "relUrl": "/pipelines/configuration/processors/aggregate/"
  },"2": {
    "doc": "anomaly_detector",
    "title": "anomaly_detector",
    "content": "# anomaly_detector The anomaly detector processor takes structured data and runs anomaly detection algorithms on fields that you can configure in that data. The data must be either an integer or a real number for the anomaly detection algorithm to detect anomalies. Deploying the aggregate processor in a pipeline before the anomaly detector processor can help you achieve the best results, as the aggregate processor automatically aggregates events by key and keeps them on the same host. For example, if you are searching for an anomaly in latencies from a specific IP address and if all the events go to the same host, then the host has more data for these events. This additional data results in better training of the machine learning (ML) algorithm, which results in better anomaly detection. ## Configuration You can configure the anomaly detector processor by specifying a key and the options for the selected mode. You can use the following options to configure the anomaly detector processor. | Name | Required | Description | :--- | :--- | :--- | `keys` | Yes | A non-ordered `List` that is used as input to the ML algorithm to detect anomalies in the values of the keys in the list. At least one key is required. | `mode` | Yes | The ML algorithm (or model) used to detect anomalies. You must provide a mode. See [random_cut_forest mode](#random_cut_forest-mode). ### Keys Keys that are used in the anomaly detector processor are present in the input event. For example, if the input event is `{\"key1\":value1, \"key2\":value2, \"key3\":value3}`, then any of the keys (such as `key1`, `key2`, `key3`) in that input event can be used as anomaly detector keys as long as their value (such as `value1`, `value2`, `value3`) is an integer or real number. ### random_cut_forest mode The random cut forest (RCF) ML algorithm is an unsupervised algorithm for detecting anomalous data points within a dataset. To detect anomalies, the anomaly detector processor uses the `random_cut_forest` mode. | Name | Description | :--- | :--- | `random_cut_forest` | Processes events using the RCF ML algorithm to detect anomalies. | RCF is an unsupervised ML algorithm for detecting anomalous data points within a dataset. Data Prepper uses RCF to detect anomalies in data by passing the values of the configured key to RCF. For example, when an event with a latency value of 11.5 is sent, the following anomaly event is generated: ```json { \"latency\": 11.5, \"deviation_from_expected\":[10.469302736820003],\"grade\":1.0} ``` In this example, `deviation_from_expected` is a list of deviations for each of the keys from their corresponding expected values, and `grade` is the anomaly grade that indicates the anomaly severity. You can configure `random_cut_forest` mode with the following options. | Name | Default value | Range | Description | :--- | :--- | :--- | :--- | `shingle_size` | `4` | 1--60 | The shingle size used in the ML algorithm. | `sample_size` | `256` | 100--2500 | The sample size used in the ML algorithm. | `time_decay` | `0.1` | 0--1.0 | The time decay value used in the ML algorithm. Used as the mathematical expression `timeDecay` divided by `SampleSize` in the ML algorithm. | `type` | `metrics` | N/A | The type of data sent to the algorithm. | `version` | `1.0` | N/A | The algorithm version number. | ## Usage To get started, create the following `pipeline.yaml` file. You can use the following pipeline configuration to look for anomalies in the `latency` field in events that are passed to the processor. Then you can use the following YAML configuration file `random_cut_forest` mode to detect anomalies: ```yaml ad-pipeline: source: ...... processor: - anomaly_detector: keys: [\"latency\"] mode: random_cut_forest: ``` When you run the anomaly detector processor, the processor extracts the value for the `latency` key, and then passes the value through the RCF ML algorithm. You can configure any key that comprises integers or real numbers as values. In the following example, you can configure `bytes` or `latency` as the key for an anomaly detector. `{\"ip\":\"1.2.3.4\", \"bytes\":234234, \"latency\":0.2}` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/anomaly-detector/",
    "relUrl": "/pipelines/configuration/processors/anomaly-detector/"
  },"3": {
    "doc": "Bounded blocking",
    "title": "Bounded blocking",
    "content": "# Bounded blocking ## Overview `Bounded blocking` is the default buffer and is memory based. The following table describes the `Bounded blocking` parameters. | Option | Required | Type | Description | --- | --- | --- | --- | buffer_size | No | Integer | The maximum number of records the buffer accepts. Default value is `12800`. | batch_size | No | Integer | The maximum number of records the buffer drains after each read. Default value is `200`. | ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/buffers/bounded-blocking/",
    "relUrl": "/pipelines/configuration/buffers/bounded-blocking/"
  },"4": {
    "doc": "Buffers",
    "title": "Buffers",
    "content": "# Buffers Buffers store data as it passes through the pipeline. If you implement a custom buffer, it can be memory based, which provides better performance, or disk based, which is larger in size. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/buffers/buffers/",
    "relUrl": "/pipelines/configuration/buffers/buffers/"
  },"5": {
    "doc": "Common use cases",
    "title": "Common use cases",
    "content": "# Common use cases You can use Data Prepper for several different purposes, including trace analytics, log analytics, Amazon S3 log analytics, and metrics ingestion. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/common-use-cases/common-use-cases/",
    "relUrl": "/common-use-cases/common-use-cases/"
  },"6": {
    "doc": "Configuring Data Prepper",
    "title": "Configuring Data Prepper",
    "content": "# Configuring Data Prepper You can customize your Data Prepper confiuration by editing the `data-prepper-config.yaml` file in your Data Prepper installation. The following configuration options are independent from pipeline configuration options. ## Data Prepper configuration Use the following options to customize your Data Prepper configuration. Option | Required | Type | Description :--- | :--- |:--- | :--- ssl | No | Boolean | Indicates whether TLS should be used for server APIs. Defaults to true. keyStoreFilePath | No | String | The path to a .jks or .p12 keystore file. Required if `ssl` is true. keyStorePassword | No | String | The password for keystore. Optional, defaults to empty string. privateKeyPassword | No | String | The password for a private key within keystore. Optional, defaults to empty string. serverPort | No | Integer | The port number to use for server APIs. Defaults to 4900. metricRegistries | No | List | The metrics registries for publishing the generated metrics. Currently supports Prometheus and Amazon CloudWatch. Defaults to Prometheus. metricTags | No | Map | A map of key-value pairs as common metric tags to metric registries. The maximum number of pairs is three. Note that `serviceName` is a reserved tag key with `DataPrepper` as the default tag value. Alternatively, administrators can set this value through the environment variable `DATAPREPPER_SERVICE_NAME`. If `serviceName` is defined in `metricTags`, that value overwrites those set through the above methods. authentication | No | Object | The authentication configuration. Valid option is `http_basic` with `username` and `password` properties. If not defined, the server does not perform authentication. processorShutdownTimeout | No | Duration | The time given to processors to clear any in-flight data and gracefully shut down. Default is 30s. sinkShutdownTimeout | No | Duration | The time given to sinks to clear any in-flight data and gracefully shut down. Default is 30s. peer_forwarder | No | Object | Peer forwarder configurations. See [Peer forwarder options](#peer-forwarder-options) for more details. circuit_breakers | No | [circuit_breakers](#circuit-breakers) | Configures a circuit breaker on incoming data. ### Peer forwarder options The following section details various configuration options for peer forwarder. #### General options for peer forwarding Option | Required | Type | Description :--- | :--- | :--- | :--- port | No | Integer | The peer forwarding server port. Valid options are between 0 and 65535. Defaults is 4994. request_timeout | No | Integer | The request timeout for the peer forwarder HTTP server in milliseconds. Default is 10000. server_thread_count | No | Integer | The number of threads used by the peer forwarder server. Default is 200. client_thread_count | No | Integer | The number of threads used by the peer forwarder client. Default is 200. max_connection_count | No | Integer | The maximum number of open connections for the peer forwarder server. Default is 500. max_pending_requests | No | Integer | The maximum number of allowed tasks in ScheduledThreadPool work queue. Default is 1024. discovery_mode | No | String | The peer discovery mode to use. Valid options are `local_node`, `static`, `dns`, or `aws_cloud_map`. Defaults to `local_node`, which processes events locally. static_endpoints | Conditionally | List | A list containing endpoints of all Data Prepper instances. Required if `discovery_mode` is set to static. domain_name | Conditionally | String | A single domain name to query DNS against. Typically, used by creating multiple DNS A Records for the same domain. Required if `discovery_mode` is set to dns. aws_cloud_map_namespace_name | Conditionally | String | Cloud Map namespace when using AWS Cloud Map service discovery. Required if `discovery_mode` is set to `aws_cloud_map`. aws_cloud_map_service_name | Conditionally | String | The Cloud Map service name when using AWS Cloud Map service discovery. Required if `discovery_mode` is set to `aws_cloud_map`. aws_cloud_map_query_parameters | No | Map | A map of key-value pairs to filter the results based on the custom attributes attached to an instance. Only instances that match all the specified key-value pairs are returned. buffer_size | No | Integer | The maximum number of unchecked records the buffer accepts. Number of unchecked records is the sum of the number of records written into the buffer and the num of in-flight records not yet checked by the Checkpointing API. Default is 512. batch_size | No | Integer | The maximum number of records the buffer returns on read. Default is 48. aws_region | Conditionally | String | The AWS region to use with ACM, S3 or AWS Cloud Map. Required if `use_acm_certificate_for_ssl` is set to true or `ssl_certificate_file` and `ssl_key_file` is AWS S3 path or `discovery_mode` is set to `aws_cloud_map`. drain_timeout | No | Duration | The wait time for the peer forwarder to complete processing data before shutdown. Default is `10s`. #### TLS/SSL options for peer forwarder Option | Required | Type | Description :--- | :--- | :--- | :--- ssl | No | Boolean | Enables TLS/SSL. Default is `true`. ssl_certificate_file | Conditionally | String | The SSL certificate chain file path or AWS S3 path. S3 path example `s3:///`. Required if `ssl` is true and `use_acm_certificate_for_ssl` is false. Defaults to `config/default_certificate.pem` which is the default certificate file. Read more about how the certificate file is generated [here](https://github.com/opensearch-project/data-prepper/tree/main/examples/certificates). ssl_key_file | Conditionally | String | The SSL key file path or AWS S3 path. S3 path example `s3:///`. Required if `ssl` is true and `use_acm_certificate_for_ssl` is false. Defaults to `config/default_private_key.pem` which is the default private key file. Read more about how the default private key file is generated [here](https://github.com/opensearch-project/data-prepper/tree/main/examples/certificates). ssl_insecure_disable_verification | No | Boolean | Disables the verification of server's TLS certificate chain. Default is false. ssl_fingerprint_verification_only | No | Boolean | Disables the verification of server's TLS certificate chain and instead verifies only the certificate fingerprint. Default is false. use_acm_certificate_for_ssl | No | Boolean | Enables TLS/SSL using certificate and private key from AWS Certificate Manager (ACM). Default is false. acm_certificate_arn | Conditionally | String | The ACM certificate ARN. The ACM certificate takes preference over S3 or a local file system certificate. Required if `use_acm_certificate_for_ssl` is set to true. acm_private_key_password | No | String | The ACM private key password that decrypts the private key. If not provided, Data Prepper generates a random password. acm_certificate_timeout_millis | No | Integer | The timeout in milliseconds for ACM to get certificates. Default is 120000. aws_region | Conditionally | String | The AWS region to use ACM, S3 or AWS Cloud Map. Required if `use_acm_certificate_for_ssl` is set to true or `ssl_certificate_file` and `ssl_key_file` is AWS S3 path or `discovery_mode` is set to `aws_cloud_map`. #### Authentication options for peer forwarder Option | Required | Type | Description :--- | :--- | :--- | :--- authentication | No | Map | The authentication method to use. Valid options are `mutual_tls` (use mTLS) or `unauthenticated` (no authentication). Default is `unauthenticated`. ### Circuit breakers Data Prepper provides a circuit breaker to help prevent exhausting Java memory. And is useful when pipelines have stateful processors as these can retain memory usage outside of the buffers. When a circuit breaker is tripped, Data Prepper rejects incoming data routing into buffers. Option | Required | Type | Description :--- | :--- |:---| :--- heap | No | [heap](#heap-circuit-breaker) | Enables a heap circuit breaker. By default, this is not enabled. #### Heap circuit breaker Configures Data Prepper to trip a circuit breaker when JVM heap reaches a specified usage threshold. Option | Required | Type | Description :--- |:---|:---| :--- usage | Yes | Bytes | Specifies the JVM heap usage at which to trip a circuit breaker. If the current Java heap usage exceeds this value then the circuit breaker will be open. This can be a value such as `6.5gb`. reset | No | Duration | After tripping the circuit breaker, no new checks are made until after this time has passed. This effectively sets the minimum time for a breaker to remain open to allow for clearing memory. Defaults to `1s`. check_interval | No | Duration | Specifies the time between checks of the heap size. Defaults to `500ms`. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/managing-data-prepper/configuring-data-prepper/",
    "relUrl": "/managing-data-prepper/configuring-data-prepper/"
  },"7": {
    "doc": "Configuring Log4j",
    "title": "Configuring Log4j",
    "content": "# Configuring Log4j You can configure logging using Log4j in Data Prepper. ## Logging Data Prepper uses [SLF4J](https://www.slf4j.org/) with a [Log4j 2 binding](https://logging.apache.org/log4j/2.x/log4j-slf4j-impl.html). For Data Prepper versions 2.0 and later, the Log4j 2 configuration file can be found and edited in `config/log4j2.properties` in the application's home directory. The default properties for Log4j 2 can be found in `log4j2-rolling.properties` in the *shared-config* directory. For Data Prepper versions before 2.0, the Log4j 2 configuration file can be overridden by setting the `log4j.configurationFile` system property when running Data Prepper. The default properties for Log4j 2 can be found in `log4j2.properties` in the *shared-config* directory. ### Example When running Data Prepper, the following command can be overridden by setting the system property `-Dlog4j.configurationFile={property_value}`, where `{property_value}` is a path to the Log4j 2 configuration file: ``` java \"-Dlog4j.configurationFile=config/custom-log4j2.properties\" -jar data-prepper-core-$VERSION.jar pipelines.yaml data-prepper-config.yaml ``` See the [Log4j 2 configuration documentation](https://logging.apache.org/log4j/2.x/manual/configuration.html) for more information about Log4j 2 configuration. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/managing-data-prepper/configuring-log4j/",
    "relUrl": "/managing-data-prepper/configuring-log4j/"
  },"8": {
    "doc": "convert_entry_type",
    "title": "convert_entry_type",
    "content": "# convert_entry_type The `convert_entry_type` processor converts a value type associated with the specified key in a event to the specified type. It is a casting processor that changes the types of some fields in events. Some data must be converted to a different type, such as an integer to a double, or a string to an integer, so that it will pass the events through condition-based processors or perform conditional routing. ## Configuration You can configure the `convert_entry_type` processor with the following options. | Option | Required | Description | :--- | :--- | :--- | `key`| Yes | Keys whose value needs to be converted to a different type. | `type` | No | Target type for the key-value pair. Possible values are `integer`, `double`, `string`, and `Boolean`. Default value is `integer`. | ## Usage To get started, create the following `pipeline.yaml` file: ```yaml type-conv-pipeline: source: ...... processor: - convert_entry_type_type: key: \"response_status\" type: \"integer\" ``` {% include copy.html %} Next, create a log file named `logs_json.log` and replace the `path` in the file source of your `pipeline.yaml` file with that filepath. For more information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). For example, before you run the `convert_entry_type` processor, if the `logs_json.log` file contains the following event record: ```json {\"message\": \"value\", \"response_status\":\"200\"} ``` The `convert_entry_type` processor converts the output received to the following output, where the type of `response_status` value changes from a string to an integer: ```json {\"message\":\"value\",\"response_status\":200} ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/convert_entry_type/",
    "relUrl": "/pipelines/configuration/processors/convert_entry_type/"
  },"9": {
    "doc": "copy_values",
    "title": "copy_values",
    "content": "# copy_values The `copy_values` processor copies values within an event and is a [mutate event]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/mutate-event/) processor. ## Configuration You can configure the `copy_values` processor with the following options. | Option | Required | Description | :--- | :--- | :--- | `entries` | Yes | A list of entries to be copied in an event. | `from_key` | Yes | The key of the entry to be copied. | `to_key` | Yes | The key of the new entry to be added. | `overwrite_if_key_exists` | No | When set to `true`, the existing value is overwritten if `key` already exists in the event. The default value is `false`. | ## Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: ...... processor: - copy_values: entries: - from_key: \"message\" to_key: \"newMessage\" overwrite_if_to_key_exists: true sink: ``` {% include copy.html %} Next, create a log file named `logs_json.log` and replace the `path` in the file source of your `pipeline.yaml` file with that filepath. For more information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). For example, before you run the `copy_values` processor, if the `logs_json.log` file contains the following event record: ```json {\"message\": \"hello\"} ``` When you run this processor, it parses the message into the following output: ```json {\"message\": \"hello\", \"newMessage\": \"hello\"} ``` > If `newMessage` already exists, its existing value is overwritten with `value`. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/copy-values/",
    "relUrl": "/pipelines/configuration/processors/copy-values/"
  },"10": {
    "doc": "Core APIs",
    "title": "Core APIs",
    "content": "# Core APIs All Data Prepper instances expose a server with some control APIs. By default, this server runs on port 4900. Some plugins, especially source plugins, may expose other servers that run on different ports. Configurations for these plugins are independent of the core API. For example, to shut down Data Prepper, you can run the following curl request: ``` curl -X POST http://localhost:4900/shutdown ``` ## APIs The following table lists the available APIs. | Name | Description | --- | --- | ```GET /list``````POST /list``` | Returns a list of running pipelines. | ```POST /shutdown``` | Starts a graceful shutdown of Data Prepper. | ```GET /metrics/prometheus``````POST /metrics/prometheus``` | Returns a scrape of Data Prepper metrics in Prometheus text format. This API is available as a `metricsRegistries` parameter in the Data Prepper configuration file `data-prepper-config.yaml` and contains `Prometheus` as part of the registry. | ```GET /metrics/sys``````POST /metrics/sys``` | Returns JVM metrics in Prometheus text format. This API is available as a `metricsRegistries` parameter in the Data Prepper configuration file `data-prepper-config.yaml` and contains `Prometheus` as part of the registry. ## Configuring the server You can configure your Data Prepper core APIs through the `data-prepper-config.yaml` file. ### SSL/TLS connection Many of the getting started guides for this project disable SSL on the endpoint: ```yaml ssl: false ``` To enable SSL on your Data Prepper endpoint, configure your `data-prepper-config.yaml` file with the following options: ```yaml ssl: true keyStoreFilePath: \"/usr/share/data-prepper/keystore.p12\" keyStorePassword: \"secret\" privateKeyPassword: \"secret\" ``` For more information about configuring your Data Prepper server with SSL, see [Server Configuration](https://github.com/opensearch-project/data-prepper/blob/main/docs/configuration.md#server-configuration). If you are using a self-signed certificate, you can add the `-k` flag to the request to quickly test core APIs with SSL. Use the following `shutdown` request to test core APIs with SSL: ``` curl -k -X POST https://localhost:4900/shutdown ``` ### Authentication The Data Prepper core APIs support HTTP basic authentication. You can set the username and password with the following configuration in the `data-prepper-config.yaml` file: ```yaml authentication: http_basic: username: \"myuser\" password: \"mys3cr3t\" ``` You can disable authentication of core endpoints using the following configuration. Use this with caution because the shutdown API and others will be accessible to anybody with network access to your Data Prepper instance. ```yaml authentication: unauthenticated: ``` ### Peer Forwarder Peer Forwarder can be configured to enable stateful aggregation across multiple Data Prepper nodes. For more information about configuring Peer Forwarder, see [Peer forwarder]({{site.url}}{{site.baseurl}}/data-prepper/managing-data-prepper/peer-forwarder/). It is supported by the `service_map_stateful`, `otel_trace_raw`, and `aggregate` processors. ### Shutdown timeouts When you run the Data Prepper `shutdown` API, the process gracefully shuts down and clears any remaining data for both the `ExecutorService` sink and `ExecutorService` processor. The default timeout for shutdown of both processes is 10 seconds. You can configure the timeout with the following optional `data-prepper-config.yaml` file parameters: ```yaml processorShutdownTimeout: \"PT15M\" sinkShutdownTimeout: 30s ``` The values for these parameters are parsed into a `Duration` object through the [Data Prepper Duration Deserializer](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-core/src/main/java/org/opensearch/dataprepper/parser/DataPrepperDurationDeserializer.java). ",
    "url": "http://localhost:4000/data-prepper/docs/latest/managing-data-prepper/core-apis/",
    "relUrl": "/managing-data-prepper/core-apis/"
  },"11": {
    "doc": "csv",
    "title": "csv",
    "content": "# csv The `csv` processor parses comma-separated values (CSVs) from the event into columns. ## Configuration The following table describes the options you can use to configure the `csv` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- source | No | String | The field in the event that will be parsed. Default value is `message`. quote_character | No | String | The character used as a text qualifier for a single column of data. Default value is `\"`. delimiter | No | String | The character separating each column. Default value is `,`. delete_header | No | Boolean | If specified, the event header (`column_names_source_key`) is deleted after the event is parsed. If there is no event header, no action is taken. Default value is true. column_names_source_key | No | String | The field in the event that specifies the CSV column names, which will be automatically detected. If there need to be extra column names, the column names are automatically generated according to their index. If `column_names` is also defined, the header in `column_names_source_key` can also be used to generate the event fields. If too few columns are specified in this field, the remaining column names are automatically generated. If too many column names are specified in this field, the CSV processor omits the extra column names. column_names | No | List | User-specified names for the CSV columns. Default value is `[column1, column2, ..., columnN]` if there are no columns of data in the CSV record and `column_names_source_key` is not defined. If `column_names_source_key` is defined, the header in `column_names_source_key` generates the event fields. If too few columns are specified in this field, the remaining column names are automatically generated. If too many column names are specified in this field, the CSV processor omits the extra column names. ## Metrics The following table describes common [Abstract processor](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/processor/AbstractProcessor.java) metrics. | Metric name | Type | Description | ------------- | ---- | -----------| `recordsIn` | Counter | Metric representing the ingress of records to a pipeline component. | `recordsOut` | Counter | Metric representing the egress of records from a pipeline component. | `timeElapsed` | Timer | Metric representing the time elapsed during execution of a pipeline component. | The `csv` processor includes the following custom metrics. **Counter** * `csvInvalidEvents`: The number of invalid events. An exception is thrown when an invalid event is parsed. An unclosed quote usually causes this exception. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/csv/",
    "relUrl": "/pipelines/configuration/processors/csv/"
  },"12": {
    "doc": "date",
    "title": "date",
    "content": "# date The `date` processor adds a default timestamp to an event, parses timestamp fields, and converts timestamp information to the International Organization for Standardization (ISO) 8601 format. This timestamp information can be used as an event timestamp. ## Configuration The following table describes the options you can use to configure the `date` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- match | Conditionally | List | List of `key` and `patterns` where patterns is a list. The list of match can have exactly one `key` and `patterns`. There is no default value. This option cannot be defined at the same time as `from_time_received`. Include multiple date processors in your pipeline if both options should be used. from_time_received | Conditionally | Boolean | A boolean that is used for adding default timestamp to event data from event metadata which is the time when source receives the event. Default value is `false`. This option cannot be defined at the same time as `match`. Include multiple date processors in your pipeline if both options should be used. destination | No | String | Field to store the timestamp parsed by date processor. It can be used with both `match` and `from_time_received`. Default value is `@timestamp`. source_timezone | No | String | Time zone used to parse dates. It is used in case the zone or offset cannot be extracted from the value. If the zone or offset are part of the value, then timezone is ignored. Find all the available timezones [the list of database time zones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List) in the **TZ database name** column. destination_timezone | No | String | Timezone used for storing timestamp in `destination` field. The available timezone values are the same as `source_timestamp`. locale | No | String | Locale is used for parsing dates. It's commonly used for parsing month names(`MMM`). It can have language, country and variant fields using IETF BCP 47 or String representation of [Locale](https://docs.oracle.com/javase/8/docs/api/java/util/Locale.html) object. For example `en-US` for IETF BCP 47 and `en_US` for string representation of Locale. Full list of locale fields which includes language, country and variant can be found [the language subtag registry](https://www.iana.org/assignments/language-subtag-registry/language-subtag-registry). Default value is `Locale.ROOT`. ## Metrics The following table describes common [Abstract processor](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/processor/AbstractProcessor.java) metrics. | Metric name | Type | Description | ------------- | ---- | -----------| `recordsIn` | Counter | Metric representing the ingress of records to a pipeline component. | `recordsOut` | Counter | Metric representing the egress of records from a pipeline component. | `timeElapsed` | Timer | Metric representing the time elapsed during execution of a pipeline component. | The `date` processor includes the following custom metrics. * `dateProcessingMatchSuccessCounter`: Returns the number of records that match with at least one pattern specified by the `match configuration` option. * `dateProcessingMatchFailureCounter`: Returns the number of records that did not match any of the patterns specified by the `patterns match` configuration option. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/date/",
    "relUrl": "/pipelines/configuration/processors/date/"
  },"13": {
    "doc": "delete_entries",
    "title": "delete_entries",
    "content": "# delete_entries The `delete_entries` processor deletes entries, such as key-value pairs, from an event. You can define the keys you want to delete in the `with-keys` field following `delete_entries` in the YAML configuration file. Those keys and their values are deleted. ## Configuration You can configure the `delete_entries` processor with the following options. | Option | Required | Description | :--- | :--- | :--- | `with_keys` | Yes | An array of keys for the entries to be deleted. | ## Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: ...... processor: - delete_entries: with_keys: [\"message\"] sink: ``` {% include copy.html %} Next, create a log file named `logs_json.log` and replace the `path` in the file source of your `pipeline.yaml` file with that filepath. For more information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). For example, before you run the `delete_entries` processor, if the `logs_json.log` file contains the following event record: ```json {\"message\": \"hello\", \"message2\": \"goodbye\"} ``` When you run the `delete_entries` processor, it parses the message into the following output: ```json {\"message2\": \"goodbye\"} ``` > If `message` does not exist in the event, then no action occurs. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/delete-entries/",
    "relUrl": "/pipelines/configuration/processors/delete-entries/"
  },"14": {
    "doc": "Dead-letter queues",
    "title": "Dead-letter queues",
    "content": "# Dead-letter queues Data Prepper pipelines support dead-letter queues (DLQs) for offloading failed events and making them accessible for analysis. As of Data Prepper 2.3, only the `s3` source supports DLQs. ## Configure a DLQ writer To configure a DLQ writer for the `s3` source, add the following to your pipeline.yaml file: ```yaml sink: opensearch: dlq: s3: bucket: \"my-dlq-bucket\" key_path_prefix: \"dlq-files/\" region: \"us-west-2\" sts_role_arn: \"arn:aws:iam::123456789012:role/dlq-role\" ``` The resulting DLQ file outputs as a JSON array of DLQ objects. Any file written to the S3 DLQ contains the following name pattern: ``` dlq-v${version}-${pipelineName}-${pluginId}-${timestampIso8601}-${uniqueId} ``` The following information is replaced in the name pattern: - `version`: The Data Prepper version. - `pipelineName`: The pipeline name indicated in pipeline.yaml. - `pluginId`: The ID of the plugin associated with the DLQ event. ## Configuration DLQ supports the following configuration options. Option | Required | Type | Description :--- | :--- | :--- | :--- bucket | Yes | String | The name of the bucket into which the DLQ outputs failed records. key_path_prefix | No | String | The `key_prefix` used in the S3 bucket. Defaults to `\"\"`. Supports time value pattern variables, such as `/%{yyyy}/%{MM}/%{dd}`, including any variables listed in the [Java DateTimeFormatter](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html). For example, when using the `/%{yyyy}/%{MM}/%{dd}` pattern, you can set `key_prefix` as `/2023/01/24`. region | No | String | The AWS Region of the S3 bucket. Defaults to `us-east-1`. sts_role_arn | No | String | The STS role the DLQ assumes in order to write to an AWS S3 bucket. Default is `null`, which uses the standard SDK behavior for credentials. To use this option, the S3 bucket must have the `S3:PutObject` permission configured. When using DLQ with an OpenSearch sink, you can configure the [max_retries]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sinks/opensearch/#configure-max_retries) option to send failed data to the DLQ when the sink reaches the maximum number of retries. ## Metrics DLQ supports the following metrics. ### Counter - `dlqS3RecordsSuccess`: Measures the number of successful records sent to S3. - `dlqS3RecordsFailed`: Measures the number of records that failed to be sent to S3. - `dlqS3RequestSuccess`: Measures the number of successful S3 requests. - `dlqS3RequestFailed`: Measures the number of failed S3 requests. ### Distribution summary - `dlqS3RequestSizeBytes`: Measures the distribution of the S3 request's payload size in bytes. ### Timer - `dlqS3RequestLatency`: Measures latency when sending each S3 request, including retries. ## DLQ objects DLQ supports the following DLQ objects: * `pluginId`: The ID of the plugin that originated the event sent to the DLQ. * `pluginName`: The name of the plugin. * `failedData` : An object that contains the failed object and its options. This object is unique to each plugin. * `pipelineName`: The name of the Data Prepper pipeline in which the event failed. * `timestamp`: The timestamp of the failures in an `ISO8601` format. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/dlq/",
    "relUrl": "/pipelines/dlq/"
  },"15": {
    "doc": "drop_events",
    "title": "drop_events",
    "content": "# drop_events The `drop_events` processor drops all the events that are passed into it. The following table describes when events are dropped and how exceptions for dropping events are handled. Option | Required | Type | Description :--- | :--- | :--- | :--- drop_when | Yes | String | Accepts a Data Prepper expression string following the [Data Prepper Expression Syntax]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/expression-syntax/). Configuring `drop_events` with `drop_when: true` drops all the events received. handle_failed_events | No | Enum | Specifies how exceptions are handled when an exception occurs while evaluating an event. Default value is `drop`, which drops the event so that it is not sent to OpenSearch. Available options are `drop`, `drop_silently`, `skip`, and `skip_silently`. For more information, see [handle_failed_events](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/drop-events-processor#handle_failed_events). ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/drop-events/",
    "relUrl": "/pipelines/configuration/processors/drop-events/"
  },"16": {
    "doc": "Expression syntax",
    "title": "Expression syntax",
    "content": "# Expression syntax The following sections provide information about expression syntax in Data Prepper. ## Supported operators Operators are listed in order of precedence (top to bottom, left to right). | Operator | Description | Associativity |----------------------|-------------------------------------------------------|---------------| `()` | Priority Expression | left-to-right | `not` `+` `-`| Unary Logical NOTUnary PositiveUnary negative | right-to-left | ``, `>=` | Relational Operators | left-to-right | `==`, `!=` | Equality Operators | left-to-right | `and`, `or` | Conditional Expression | left-to-right | ## Reserved for possible future functionality Reserved symbol set: `^`, `*`, `/`, `%`, `+`, `-`, `xor`, `=`, `+=`, `-=`, `*=`, `/=`, `%=`, `++`, `--`, `${}` ## Set initializer The set initializer defines a set or term and/or expressions. ### Examples The following are examples of set initializer syntax. #### HTTP status codes ``` {200, 201, 202} ``` #### HTTP response payloads ``` {\"Created\", \"Accepted\"} ``` #### Handle multiple event types with different keys ``` {/request_payload, /request_message} ``` ## Priority expression A priority expression identifies an expression that will be evaluated at the highest priority level. A priority expression must contain an expression or value; empty parentheses are not supported. ### Example ``` /is_cool == (/name == \"Steven\") ``` ## Relational operators Relational operators are used to test the relationship of two numeric values. The operands must be numbers or JSON Pointers that resolve to numbers. ### Syntax ``` > >= ``` ### Example ``` /status_code >= 200 and /status_code == != ``` ### Examples ``` /is_cool == true 3.14 != /status_code {1, 2} == /event/set_property ``` ## Using equality operators to check for a JSON Pointer Equality operators can also be used to check whether a JSON Pointer exists by comparing the value with `null`. ### Syntax ``` == null != null null == null != ``` ### Example ``` /response == null null != /response ``` #### Conditional expression A conditional expression is used to chain together multiple expressions and/or values. #### Syntax ``` and or not ``` ### Example ``` /status_code == 200 and /message == \"Hello world\" /status_code == 200 or /status_code == 202 not /status_code in {200, 202} /response == null /response != null ``` ## Definitions This section provides expression definitions. ### Literal A literal is a fundamental value that has no children: - Float: Supports values from 3.40282347 &times; 1038 to 1.40239846 &times; 10&minus;45. - Integer: Supports values from &minus;2,147,483,648 to 2,147,483,647. - Boolean: Supports true or false. - JSON Pointer: See the [JSON Pointer](#json-pointer) section for details. - String: Supports valid Java strings. - Null: Supports null check to see whether a JSON Pointer exists. ### Expression string An expression string takes the highest priority in a Data Prepper expression and only supports one expression string resulting in a return value. An _expression string_ is not the same as an _expression_. ### Statement A statement is the highest-priority component of an expression string. ### Expression An expression is a generic component that contains a _Primary_ or an _Operator_. Expressions may contain expressions. An expression's imminent children can contain 01 _Operators_. ### Primary - _Set_ - _Priority Expression_ - _Literal_ ### Operator An operator is a hardcoded token that identifies the operation used in an _expression_. ### JSON Pointer A JSON Pointer is a literal used to reference a value within an event and provided as context for an _expression string_. JSON Pointers are identified by a leading `/` containing alphanumeric characters or underscores, delimited by `/`. JSON Pointers can use an extended character set if wrapped in double quotes (`\"`) using the escape character `\\`. Note that JSON Pointers require `~` and `/` characters, which should be used as part of the path and not as a delimiter that needs to be escaped. The following are examples of JSON Pointers: - `~0` representing `~` - `~1` representing `/` #### Shorthand syntax (Regex, `\\w` = `[A-Za-z_]`) ``` /\\w+(/\\w+)* ``` #### Example of shorthand The following is an example of shorthand: ``` /Hello/World/0 ``` #### Example of escaped syntax The following is an example of escaped syntax: ``` \"/(/)*\" ``` #### Example of an escaped JSON Pointer The following is an example of an escaped JSON Pointer: ``` # Path # { \"Hello - 'world/\" : [{ \"\\\"JsonPointer\\\"\": true }] } \"/Hello - 'world\\//0/\\\"JsonPointer\\\"\" ``` ## White space White space is **optional** surrounding relational operators, regex equality operators, equality operators, and commas. White space is **required** surrounding set initializers, priority expressions, set operators, and conditional expressions. | Operator | Description | White space required |  Valid examples |  Invalid examples |----------------------|--------------------------|----------------------|----------------------------------------------------------------|---------------------------------------| `{}` | Set initializer | Yes | `/status in {200}` | `/status in{200}` | `()` | Priority expression | Yes | `/a==(/b==200)``/a in ({200})` | `/status in({200})` | `in`, `not in` | Set operators | Yes | `/a in {200}``/a not in {400}` | `/a in{200, 202}``/a not in{400}` | ``, `>=` | Relational operators | No | `/status `/status>=300` | | `=~`, `!~` | Regex equality pperators | No | `/msg =~ \"^\\w*$\"``/msg=~\"^\\w*$\"` | | `==`, `!=` | Equality operators | No | `/status == 200``/status_code==200` | | `and`, `or`, `not` | Conditional operators | Yes | `/a200` | `/b200` | `,` | Set value delimiter | No | `/a in {200, 202}``/a in {200,202}``/a in {200 , 202}` | `/a in {200,}` | ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/expression-syntax/",
    "relUrl": "/pipelines/expression-syntax/"
  },"17": {
    "doc": "file sink",
    "title": "file sink",
    "content": "# file sink ## Overview You can use the `file` sink to create a flat file output. The following table describes options you can configure for the `file` sink. Option | Required | Type | Description :--- | :--- | :--- | :--- path | Yes | String | Path for the output file (e.g. `logs/my-transformed-log.log`). ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sinks/file/",
    "relUrl": "/pipelines/configuration/sinks/file/"
  },"18": {
    "doc": "Getting started",
    "title": "Getting started",
    "content": "# Getting started with Data Prepper Data Prepper is an independent component, not an OpenSearch plugin, that converts data for use with OpenSearch. It's not bundled with the all-in-one OpenSearch installation packages. If you are migrating from Open Distro Data Prepper, see [Migrating from Open Distro]({{site.url}}{{site.baseurl}}/data-prepper/migrate-open-distro/). {: .note} ## 1. Installing Data Prepper There are two ways to install Data Prepper: you can run the Docker image or build from source. The easiest way to use Data Prepper is by running the Docker image. We suggest that you use this approach if you have [Docker](https://www.docker.com) available. Run the following command: ``` docker pull opensearchproject/data-prepper:latest ``` {% include copy.html %} If you have special requirements that require you to build from source, or if you want to contribute, see the [Developer Guide](https://github.com/opensearch-project/data-prepper/blob/main/docs/developer_guide.md). ## 2. Configuring Data Prepper Two configuration files are required to run a Data Prepper instance. Optionally, you can configure a Log4j 2 configuration file. See [Configuring Log4j]({{site.url}}{{site.baseurl}}/data-prepper/managing-data-prepper/configuring-log4j/) for more information. The following list describes the purpose of each configuration file: * `pipelines.yaml`: This file describes which data pipelines to run, including sources, processors, and sinks. * `data-prepper-config.yaml`: This file contains Data Prepper server settings that allow you to interact with exposed Data Prepper server APIs. * `log4j2-rolling.properties` (optional): This file contains Log4j 2 configuration options and can be a JSON, YAML, XML, or .properties file type. For Data Prepper versions earlier than 2.0, the `.jar` file expects the pipeline configuration file path to be followed by the server configuration file path. See the following configuration path example: ``` java -jar data-prepper-core-$VERSION.jar pipelines.yaml data-prepper-config.yaml ``` Optionally, you can add `\"-Dlog4j.configurationFile=config/log4j2.properties\"` to the command to pass a custom Log4j 2 configuration file. If you don't provide a properties file, Data Prepper defaults to the `log4j2.properties` file in the `shared-config` directory. Starting with Data Prepper 2.0, you can launch Data Prepper by using the following `data-prepper` script that does not require any additional command line arguments: ``` bin/data-prepper ``` Configuration files are read from specific subdirectories in the application's home directory: 1. `pipelines/`: Used for pipeline configurations. Pipeline configurations can be written in one or more YAML files. 2. `config/data-prepper-config.yaml`: Used for the Data Prepper server configuration. You can supply your own pipeline configuration file path followed by the server configuration file path. However, this method will not be supported in a future release. See the following example: ``` bin/data-prepper pipelines.yaml data-prepper-config.yaml ``` The Log4j 2 configuration file is read from the `config/log4j2.properties` file located in the application's home directory. To configure Data Prepper, see the following information for each use case: * [Trace analytics]({{site.url}}{{site.baseurl}}/data-prepper/common-use-cases/trace-analytics/): Learn how to collect trace data and customize a pipeline that ingests and transforms that data. * [Log analytics]({{site.url}}{{site.baseurl}}/data-prepper/common-use-cases/log-analytics/): Learn how to set up Data Prepper for log observability. ## 3. Defining a pipeline Create a Data Prepper pipeline file named `pipelines.yaml` using the following configuration: ```yml simple-sample-pipeline: workers: 2 delay: \"5000\" source: random: sink: - stdout: ``` {% include copy.html %} ## 4. Running Data Prepper Run the following command with your pipeline configuration YAML. ```bash docker run --name data-prepper \\ -v /${PWD}/pipelines.yaml:/usr/share/data-prepper/pipelines/pipelines.yaml \\ opensearchproject/data-prepper:latest ``` {% include copy.html %} The example pipeline configuration above demonstrates a simple pipeline with a source (`random`) sending data to a sink (`stdout`). For examples of more advanced pipeline configurations, see [Pipelines]({{site.url}}{{site.baseurl}}/clients/data-prepper/pipelines/). After starting Data Prepper, you should see log output and some UUIDs after a few seconds: ```yml 2021-09-30T20:19:44,147 [main] INFO com.amazon.dataprepper.pipeline.server.DataPrepperServer - Data Prepper server running at :4900 2021-09-30T20:19:44,681 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:45,183 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:45,687 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:46,191 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:46,694 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:47,200 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:49,181 [simple-test-pipeline-processor-worker-1-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker - simple-test-pipeline Worker: Processing 6 records from buffer 07dc0d37-da2c-447e-a8df-64792095fb72 5ac9b10a-1d21-4306-851a-6fb12f797010 99040c79-e97b-4f1d-a70b-409286f2a671 5319a842-c028-4c17-a613-3ef101bd2bdd e51e700e-5cab-4f6d-879a-1c3235a77d18 b4ed2d7e-cf9c-4e9d-967c-b18e8af35c90 ``` The remainder of this page provides examples for running Data Prepper from the Docker image. If you built it from source, refer to the [Developer Guide](https://github.com/opensearch-project/data-prepper/blob/main/docs/developer_guide.md) for more information. However you configure your pipeline, you'll run Data Prepper the same way. You run the Docker image and modify both the `pipelines.yaml` and `data-prepper-config.yaml` files. For Data Prepper 2.0 or later, use this command: ``` docker run --name data-prepper -p 4900:4900 -v ${PWD}/pipelines.yaml:/usr/share/data-prepper/pipelines/pipelines.yaml -v ${PWD}/data-prepper-config.yaml:/usr/share/data-prepper/config/data-prepper-config.yaml opensearchproject/data-prepper:latest ``` {% include copy.html %} For Data Prepper versions earlier than 2.0, use this command: ``` docker run --name data-prepper -p 4900:4900 -v ${PWD}/pipelines.yaml:/usr/share/data-prepper/pipelines.yaml -v ${PWD}/data-prepper-config.yaml:/usr/share/data-prepper/data-prepper-config.yaml opensearchproject/data-prepper:1.x ``` {% include copy.html %} Once Data Prepper is running, it processes data until it is shut down. Once you are done, shut it down with the following command: ``` POST /shutdown ``` {% include copy-curl.html %} ### Additional configurations For Data Prepper 2.0 or later, the Log4j 2 configuration file is read from `config/log4j2.properties` in the application's home directory. By default, it uses `log4j2-rolling.properties` in the *shared-config* directory. For Data Prepper 1.5 or earlier, optionally add `\"-Dlog4j.configurationFile=config/log4j2.properties\"` to the command if you want to pass a custom log4j2 properties file. If no properties file is provided, Data Prepper defaults to the log4j2.properties file in the *shared-config* directory. ## Next steps Trace analytics is an important Data Prepper use case. If you haven't yet configured it, see [Trace analytics]({{site.url}}{{site.baseurl}}/data-prepper/common-use-cases/trace-analytics/). Log ingestion is also an important Data Prepper use case. To learn more, see [Log analytics]({{site.url}}{{site.baseurl}}/data-prepper/common-use-cases/log-analytics/). To learn how to run Data Prepper with a Logstash configuration, see [Migrating from Logstash]({{site.url}}{{site.baseurl}}/data-prepper/migrating-from-logstash-data-prepper/). For information on how to monitor Data Prepper, see [Monitoring]({{site.url}}{{site.baseurl}}/data-prepper/managing-data-prepper/monitoring/). ## More examples For more examples of Data Prepper, see [examples](https://github.com/opensearch-project/data-prepper/tree/main/examples/) in the Data Prepper repo. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/getting-started/",
    "relUrl": "/getting-started/"
  },"19": {
    "doc": "grok",
    "title": "grok",
    "content": "# grok The `Grok` processor takes unstructured data and utilizes pattern matching to structure and extract important keys. ## Configuration The following table describes options you can use with the `Grok` processor to structure your data and make your data easier to query. Option | Required | Type | Description :--- | :--- | :--- | :--- match | No | Map | Specifies which keys to match specific patterns against. Default value is an empty body. keep_empty_captures | No | Boolean | Enables preserving `null` captures. Default value is `false`. named_captures_only | No | Boolean | Specifies whether to keep only named captures. Default value is `true`. break_on_match | No | Boolean | Specifies whether to match all patterns or stop once the first successful match is found. Default value is `true`. keys_to_overwrite | No | List | Specifies which existing keys will be overwritten if there is a capture with the same key value. Default value is `[]`. pattern_definitions | No | Map | Allows for custom pattern use inline. Default value is an empty body. patterns_directories | No | List | Specifies the path of directories that contain customer pattern files. Default value is an empty list. pattern_files_glob | No | String | Specifies which pattern files to use from the directories specified for `pattern_directories`. Default value is `*`. target_key | No | String | Specifies a parent-level key used to store all captures. Default value is `null`. timeout_millis | No | Integer | The maximum amount of time during which matching occurs. Setting to `0` disables the timeout. Default value is `30,000`. ## Metrics The following table describes common [Abstract processor](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/processor/AbstractProcessor.java) metrics. | Metric name | Type | Description | ------------- | ---- | -----------| `recordsIn` | Counter | Metric representing the ingress of records to a pipeline component. | `recordsOut` | Counter | Metric representing the egress of records from a pipeline component. | `timeElapsed` | Timer | Metric representing the time elapsed during execution of a pipeline component. | The `Grok` processor includes the following custom metrics. ### Counter * `grokProcessingMismatch`: Records the number of records that did not match any of the patterns specified in the match field. * `grokProcessingMatch`: Records the number of records that matched at least one pattern from the `match` field. * `grokProcessingErrors`: Records the total number of record processing errors. * `grokProcessingTimeouts`: Records the total number of records that timed out while matching. ### Timer * `grokProcessingTime`: The time taken by individual records to match against patterns from `match`. The `avg` metric is the most useful metric for this timer because it provides you with an average value of the time it takes records to match. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/grok/",
    "relUrl": "/pipelines/configuration/processors/grok/"
  },"20": {
    "doc": "http_source",
    "title": "http_source",
    "content": "# http_source `http_source` is a source plugin that supports HTTP. Currently, `http_source` only supports the JSON UTF-8 codec for incoming requests, such as `[{\"key1\": \"value1\"}, {\"key2\": \"value2\"}]`. The following table describes options you can use to configure the `http_source` source. Option | Required | Type | Description :--- | :--- | :--- | :--- port | No | Integer | The port that the source is running on. Default value is `2021`. Valid options are between `0` and `65535`. health_check_service | No | Boolean | Enables the health check service on the `/health` endpoint on the defined port. Default value is `false`. unauthenticated_health_check | No | Boolean | Determines whether or not authentication is required on the health check endpoint. Data Prepper ignores this option if no authentication is defined. Default value is `false`. request_timeout | No | Integer | The request timeout, in milliseconds. Default value is `10000`. thread_count | No | Integer | The number of threads to keep in the ScheduledThreadPool. Default value is `200`. max_connection_count | No | Integer | The maximum allowed number of open connections. Default value is `500`. max_pending_requests | No | Integer | The maximum allowed number of tasks in the `ScheduledThreadPool` work queue. Default value is `1024`. authentication | No | Object | An authentication configuration. By default, this creates an unauthenticated server for the pipeline. This uses pluggable authentication for HTTPS. To use basic authentication define the `http_basic` plugin with a `username` and `password`. To provide customer authentication, use or create a plugin that implements [ArmeriaHttpAuthenticationProvider](https://github.com/opensearch-project/data-prepper/blob/1.2.0/data-prepper-plugins/armeria-common/src/main/java/com/amazon/dataprepper/armeria/authentication/ArmeriaHttpAuthenticationProvider.java). ssl | No | Boolean | Enables TLS/SSL. Default value is false. ssl_certificate_file | Conditionally | String | SSL certificate chain file path or Amazon Simple Storage Service (Amazon S3) path. Amazon S3 path example `s3:///`. Required if `ssl` is set to true and `use_acm_certificate_for_ssl` is set to false. ssl_key_file | Conditionally | String | SSL key file path or Amazon S3 path. Amazon S3 path example `s3:///`. Required if `ssl` is set to true and `use_acm_certificate_for_ssl` is set to false. use_acm_certificate_for_ssl | No | Boolean | Enables a TLS/SSL using certificate and private key from AWS Certificate Manager (ACM). Default value is false. acm_certificate_arn | Conditionally | String | The ACM certificate Amazon Resource Name (ARN). The ACM certificate takes preference over Amazon S3 or a local file system certificate. Required if `use_acm_certificate_for_ssl` is set to true. acm_private_key_password | No | String | ACM private key password that decrypts the private key. If not provided, Data Prepper generates a random password. acm_certificate_timeout_millis | No | Integer | Timeout, in milliseconds, that ACM takes to get certificates. Default value is 120000. aws_region | Conditionally | String | AWS region used by ACM or Amazon S3. Required if `use_acm_certificate_for_ssl` is set to true or `ssl_certificate_file` and `ssl_key_file` is the Amazon S3 path. ## Metrics The `http_source` source includes the following metrics. ### Counters - `requestsReceived`: Measures the total number of requests received by the `/log/ingest` endpoint. - `requestsRejected`: Measures the total number of requests rejected (429 response status code) by the HTTP Source plugin. - `successRequests`: Measures the total number of requests successfully processed (200 response status code) the by HTTP Source plugin. - `badRequests`: Measures the total number of requests with either an invalid content type or format processed by the HTTP Source plugin (400 response status code). - `requestTimeouts`: Measures the total number of requests that time out in the HTTP source server (415 response status code). - `requestsTooLarge`: Measures the total number of requests where the size of the event is larger than the buffer capacity (413 response status code). - `internalServerError`: Measures the total number of requests processed by the HTTP Source with a custom exception type (500 response status code). ### Timers - `requestProcessDuration`: Measures the latency of requests processed by the HTTP Source plugin in seconds. ### Distribution summaries - `payloadSize`: Measures the incoming request payload size in bytes. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sources/http-source/",
    "relUrl": "/pipelines/configuration/sources/http-source/"
  },"21": {
    "doc": "Data Prepper",
    "title": "Data Prepper",
    "content": "# Data Prepper Data Prepper is a server-side data collector capable of filtering, enriching, transforming, normalizing, and aggregating data for downstream analytics and visualization. Data Prepper lets users build custom pipelines to improve the operational view of applications. Two common uses for Data Prepper are trace and log analytics. [Trace analytics]({{site.url}}{{site.baseurl}}/observability-plugin/trace/index/) can help you visualize the flow of events and identify performance problems, and [log analytics]({{site.url}}{{site.baseurl}}/observability-plugin/log-analytics/) can improve searching, analyzing and provide insights into your application. ## Concepts Data Prepper includes one or more **pipelines** that collect and filter data based on the components set within the pipeline. Each component is pluggable, enabling you to use your own custom implementation of each component. These components include the following: - One [source](#source) - One or more [sinks](#sink) - (Optional) One [buffer](#buffer) - (Optional) One or more [processors](#processor) A single instance of Data Prepper can have one or more pipelines. Each pipeline definition contains two required components: **source** and **sink**. If buffers and processors are missing from the Data Prepper pipeline, Data Prepper uses the default buffer and a no-op processor. ### Source Source is the input component that defines the mechanism through which a Data Prepper pipeline will consume events. A pipeline can have only one source. The source can consume events either by receiving the events over HTTP or HTTPS or by reading from external endpoints like OTeL Collector for traces and metrics and Amazon Simple Storage Service (Amazon S3). Sources have their own configuration options based on the format of the events (such as string, JSON, Amazon CloudWatch logs, or open telemetry trace). The source component consumes events and writes them to the buffer component. ### Buffer The buffer component acts as the layer between the source and the sink. Buffer can be either in-memory or disk based. The default buffer uses an in-memory queue called `bounded_blocking` that is bounded by the number of events. If the buffer component is not explicitly mentioned in the pipeline configuration, Data Prepper uses the default `bounded_blocking`. ### Sink Sink is the output component that defines the destination(s) to which a Data Prepper pipeline publishes events. A sink destination could be a service, such as OpenSearch or Amazon S3, or another Data Prepper pipeline. When using another Data Prepper pipeline as the sink, you can chain multiple pipelines together based on the needs of the data. Sink contains its own configuration options based on the destination type. ### Processor Processors are units within the Data Prepper pipeline that can filter, transform, and enrich events using your desired format before publishing the record to the sink component. The processor is not defined in the pipeline configuration; the events publish in the format defined in the source component. You can have more than one processor within a pipeline. When using multiple processors, the processors are run in the order they are defined inside the pipeline specification. ## Sample pipeline configurations To understand how all pipeline components function within a Data Prepper configuration, see the following examples. Each pipeline configuration uses a `yaml` file format. ### Minimal component This pipeline configuration reads from the file source and writes to another file in the same path. It uses the default options for the buffer and processor. ```yml sample-pipeline: source: file: path: sink: - file: path: ``` ### All components The following pipeline uses a source that reads string events from the `input-file`. The source then pushes the data to the buffer, bounded by a max size of `1024`. The pipeline is configured to have `4` workers, each of them reading a maximum of `256` events from the buffer for every `100 milliseconds`. Each worker runs the `string_converter` processor and writes the output of the processor to the `output-file`. ```yml sample-pipeline: workers: 4 #Number of workers delay: 100 # in milliseconds, how often the workers should run source: file: path: buffer: bounded_blocking: buffer_size: 1024 # max number of events the buffer will accept batch_size: 256 # max number of events the buffer will drain for each read processor: - string_converter: upper_case: true sink: - file: path: ``` ## Next steps To get started building your own custom pipelines with Data Prepper, see [Getting started]({{site.url}}{{site.baseurl}}/clients/data-prepper/get-started/). ",
    "url": "http://localhost:4000/data-prepper/docs/latest/",
    "relUrl": "/"
  },"22": {
    "doc": "key_value",
    "title": "key_value",
    "content": "# key_value You can use the `key_value` processor to parse the specified field into key-value pairs. You can customize the `key_value` processor to parse field information with the following options. The type for each of the following options is `string`. | Option | Description | Example | :--- | :--- | :--- | source | The message field to be parsed. Optional. Default value is `message`. | If `source` is `\"message1\"`, `{\"message1\": {\"key1=value1\"}, \"message2\": {\"key2=value2\"}}` parses into `{\"message1\": {\"key1=value1\"}, \"message2\": {\"key2=value2\"}, \"parsed_message\": {\"key1\": \"value1\"}}`. | destination | The destination field for the parsed source. The parsed source overwrites the preexisting data for that key. Optional. Default value is `parsed_message`. | If `destination` is `\"parsed_data\"`, `{\"message\": {\"key1=value1\"}}` parses into `{\"message\": {\"key1=value1\"}, \"parsed_data\": {\"key1\": \"value1\"}}`. | field_delimiter_regex | A regular expression specifying the delimiter that separates key-value pairs. Special regular expression characters such as `[` and `]` must be escaped with `\\\\`. Cannot be defined at the same time as `field_split_characters`. Optional. If this option is not defined, `field_split_characters` is used. | If `field_delimiter_regex` is `\"&\\\\{2\\\\}\"`, `{\"key1=value1&&key2=value2\"}` parses into `{\"key1\": \"value1\", \"key2\": \"value2\"}`. | field_split_characters | A string of characters specifying the delimeter that separates key-value pairs. Special regular expression characters such as `[` and `]` must be escaped with `\\\\`. Cannot be defined at the same time as `field_delimiter_regex`. Optional. Default value is `&`. | If `field_split_characters` is `\"&&\"`, `{\"key1=value1&&key2=value2\"}` parses into `{\"key1\": \"value1\", \"key2\": \"value2\"}`. | key_value_delimiter_regex | A regular expression specifying the delimiter that separates the key and value within a key-value pair. Special regular expression characters such as `[` and `]` must be escaped with `\\\\`. This option cannot be defined at the same time as `value_split_characters`. Optional. If this option is not defined, `value_split_characters` is used. | If `key_value_delimiter_regex` is `\"=\\\\{2\\\\}\"`, `{\"key1==value1\"}` parses into `{\"key1\": \"value1\"}`. | value_split_characters | A string of characters specifying the delimiter that separates the key and value within a key-value pair. Special regular expression characters such as `[` and `]` must be escaped with `\\\\`. Cannot be defined at the same time as `key_value_delimiter_regex`. Optional. Default value is `=`. | If `value_split_characters` is `\"==\"`, `{\"key1==value1\"}` parses into `{\"key1\": \"value1\"}`. | non_match_value | When a key-value pair cannot be successfully split, the key-value pair is placed in the `key` field, and the specified value is placed in the `value` field. Optional. Default value is `null`. | `key1value1&key2=value2` parses into `{\"key1value1\": null, \"key2\": \"value2\"}`. | prefix | A prefix to append before all keys. Optional. Default value is an empty string. | If `prefix` is `\"custom\"`, `{\"key1=value1\"}` parses into `{\"customkey1\": \"value1\"}`.| delete_key_regex | A regular expression specifying the characters to delete from the key. Special regular expression characters such as `[` and `]` must be escaped with `\\\\`. Cannot be an empty string. Optional. No default value. | If `delete_key_regex` is `\"\\s\"`, `{\"key1 =value1\"}` parses into `{\"key1\": \"value1\"}`. | delete_value_regex | A regular expression specifying the characters to delete from the value. Special regular expression characters such as `[` and `]` must be escaped with `\\\\`. Cannot be an empty string. Optional. No default value. | If `delete_value_regex` is `\"\\s\"`, `{\"key1=value1 \"}` parses into `{\"key1\": \"value1\"}`. | ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/key-value/",
    "relUrl": "/pipelines/configuration/processors/key-value/"
  },"23": {
    "doc": "list_to_map",
    "title": "list_to_map",
    "content": "# list_to_map The `list_to_map` processor converts a list of objects from an event, where each object contains a `key` field, into a map of target keys. ## Configuration The following table describes the configuration options used to generate target keys for the mappings. Option | Required | Type | Description :--- | :--- | :--- | :--- `key` | Yes | String | The key of the fields to be extracted as keys in the generated mappings. `source` | Yes | String | The list of objects with `key` fields to be converted into keys for the generated map. `target` | No | String | The target for the generated map. When not specified, the generated map will be placed in the root node. `value_key` | No | String | When specified, values given a `value_key` in objects contained in the source list will be extracted and converted into the value specified by this option based on the generated map. When not specified, objects contained in the source list retain their original value when mapped. `flatten` | No | Boolean | When `true`, values in the generated map output flatten into single items based on the `flattened_element`. Otherwise, objects mapped to values from the generated map appear as lists. `flattened_element` | Conditionally | String | The element to keep, either `first` or `last`, when `flatten` is set to `true`. ## Usage The following example shows how to test the usage of the `list_to_map` processor before using the processor on your own source. Create a source file named `logs_json.log`. Because the `file` source reads each line in the `.log` file as an event, the object list appears as one line even though it contains multiple objects: ```json {\"mylist\":[{\"name\":\"a\",\"value\":\"val-a\"},{\"name\":\"b\",\"value\":\"val-b1\"},{\"name\":\"b\", \"value\":\"val-b2\"},{\"name\":\"c\",\"value\":\"val-c\"}]} ``` {% include copy.html %} Next, create a `pipeline.yaml` file that uses the `logs_json.log` file as the `source` by pointing to the `.log` file's correct path: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" value_key: \"value\" flatten: true sink: - stdout: ``` {% include copy.html %} Run the pipeline. If successful, the processor returns the generated map with objects mapped according to their `value_key`. Similar to the original source, which contains one line and therefore one event, the processor returns the following JSON as one line. For readability, the following example and all subsequent JSON examples have been adjusted to span multiple lines: ```json { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"a\": \"val-a\", \"b\": \"val-b1\", \"c\": \"val-c\" } ``` ### Example: Maps set to `target` The following example `pipeline.yaml` file shows the `list_to_map` processor when set to a specified target, `mymap`: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" target: \"mymap\" value_key: \"value\" flatten: true sink: - stdout: ``` {% include copy.html %} The generated map appears under the target key: ```json { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"mymap\": { \"a\": \"val-a\", \"b\": \"val-b1\", \"c\": \"val-c\" } } ``` ### Example: No `value_key` specified The follow example `pipeline.yaml` file shows the `list_to_map` processor with no `value_key` specified. Because `key` is set to `name`, the processor extracts the object names to use as keys in the map. ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" flatten: true sink: - stdout: ``` {% include copy.html %} The values from the generated map appear as original objects from the `.log` source, as shown in the following example response: ```json { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"a\": { \"name\": \"a\", \"value\": \"val-a\" }, \"b\": { \"name\": \"b\", \"value\": \"val-b1\" }, \"c\": { \"name\": \"c\", \"value\": \"val-c\" } } ``` ### Example: `flattened_element` set to `last` The following example `pipeline.yaml` file sets the `flattened_element` to last, therefore flattening the processor output based on each value's last element: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" target: \"mymap\" value_key: \"value\" flatten: true flattened_element: \"last\" sink: - stdout: ``` {% include copy.html %} The processor maps object `b` to value `val-b2` because `val-b2` is the last element in object `b`, as shown in the following output: ```json { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"a\": \"val-a\", \"b\": \"val-b2\", \"c\": \"val-c\" } ``` ### Example: `flatten` set to false The following example `pipeline.yaml` file sets `flatten` to `false`, causing the processor to output values from the generated map as a list: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - list_to_map: key: \"name\" source: \"mylist\" target: \"mymap\" value_key: \"value\" flatten: false sink: - stdout: ``` {% include copy.html %} Some objects in the response may have more than one element in their values, as shown in the following response: ```json { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" } ], \"a\": [ \"val-a\" ], \"b\": [ \"val-b1\", \"val-b2\" ], \"c\": [ \"val-c\" ] } ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/list-to-map/",
    "relUrl": "/pipelines/configuration/processors/list-to-map/"
  },"24": {
    "doc": "Log analytics",
    "title": "Log analytics",
    "content": "# Log analytics Data Prepper is an extendable, configurable, and scalable solution for log ingestion into OpenSearch and Amazon OpenSearch Service. Data Prepper supports receiving logs from [Fluent Bit](https://fluentbit.io/) through the [HTTP Source](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-plugins/http-source/README.md) and processing those logs with a [Grok Processor](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-plugins/grok-processor/README.md) before ingesting them into OpenSearch through the [OpenSearch sink](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-plugins/opensearch/README.md). The following image shows all of the components used for log analytics with Fluent Bit, Data Prepper, and OpenSearch. ![Log analytics component]({{site.url}}{{site.baseurl}}/images/data-prepper/log-analytics/log-analytics-components.jpg) In the application environment, run Fluent Bit. Fluent Bit can be containerized through Kubernetes, Docker, or Amazon Elastic Container Service (Amazon ECS). You can also run Fluent Bit as an agent on Amazon Elastic Compute Cloud (Amazon EC2). Configure the [Fluent Bit http output plugin](https://docs.fluentbit.io/manual/pipeline/outputs/http) to export log data to Data Prepper. Then deploy Data Prepper as an intermediate component and configure it to send the enriched log data to your OpenSearch cluster. From there, use OpenSearch Dashboards to perform more intensive visualization and analysis. ## Log analytics pipeline Log analytics pipelines in Data Prepper are extremely customizable. The following image shows a simple pipeline. ![Log analytics component]({{site.url}}{{site.baseurl}}/images/data-prepper/log-analytics/log-ingestion-pipeline.jpg) ### HTTP source The [HTTP Source](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-plugins/http-source/README.md) accepts log data from Fluent Bit. This source accepts log data in a JSON array format and supports industry-standard encryption in the form of TLS/HTTPS and HTTP basic authentication. ### Processor Data Prepper 1.2 and above come with a [Grok Processor](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-plugins/grok-processor/README.md). The Grok Processor is an invaluable tool for structuring and extracting important fields from your logs, making them more queryable. The Grok Processor comes with a wide variety of [default patterns](https://github.com/thekrakken/java-grok/blob/master/src/main/resources/patterns/patterns) that match common log formats like Apache logs or syslogs, but it can easily accept any custom patterns that cater to your specific log format. For more information about Grok features, see the documentation. ### Sink There is a generic sink that writes data to OpenSearch as the destination. The [OpenSearch sink](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-plugins/opensearch/README.md) has configuration options related to an OpenSearch cluster, like endpoint, SSL/username, index name, index template, and index state management. ## Pipeline configuration The following sections discuss pipeline configuration. ### Example pipeline with SSL and basic authentication enabled This example pipeline configuration comes with SSL and basic authentication enabled for the `http-source`: ```yaml log-pipeline: source: http: ssl_certificate_file: \"/full/path/to/certfile.crt\" ssl_key_file: \"/full/path/to/keyfile.key\" authentication: http_basic: username: \"myuser\" password: \"mys3cret\" processor: - grok: match: # This will match logs with a \"log\" key against the COMMONAPACHELOG pattern (ex: { \"log\": \"actual apache log...\" } ) # You should change this to match what your logs look like. See the grok documenation to get started. log: [ \"%{COMMONAPACHELOG}\" ] sink: - opensearch: hosts: [ \"https://localhost:9200\" ] # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 # Since we are grok matching for apache logs, it makes sense to send them to an OpenSearch index named apache_logs. # You should change this to correspond with how your OpenSearch indexes are set up. index: apache_logs ``` This pipeline configuration is an example of Apache log ingestion. Don't forget that you can easily configure the Grok Processor for your own custom logs. You will need to modify the configuration for your OpenSearch cluster. The following are the main changes you need to make: * `hosts`  Set to your hosts. * `index`  Change this to the OpenSearch index to which you want to send logs. * `username`  Provide your OpenSearch username. * `password`  Provide your OpenSearch password. * `aws_sigv4`  If you use Amazon OpenSearch Service with AWS signing, set this to true. It will sign requests with the default AWS credentials provider. * `aws_region`  If you use Amazon OpenSearch Service with AWS signing, set this value to the AWS Region in which your cluster is hosted. ## Fluent Bit You will need to run Fluent Bit in your service environment. See [Getting Started with Fluent Bit](https://docs.fluentbit.io/manual/installation/getting-started-with-fluent-bit) for installation instructions. Ensure that you can configure the [Fluent Bit http output plugin](https://docs.fluentbit.io/manual/pipeline/outputs/http) to your Data Prepper HTTP source. The following is an example `fluent-bit.conf` that tails a log file named `test.log` and forwards it to a locally running Data Prepper HTTP source, which runs by default on port 2021. Note that you should adjust the file `path`, output `Host`, and `Port` according to how and where you have Fluent Bit and Data Prepper running. ### Example: Fluent Bit file without SSL and basic authentication enabled The following is an example `fluent-bit.conf` file without SSL and basic authentication enabled on the HTTP source: ``` [INPUT] name tail refresh_interval 5 path test.log read_from_head true [OUTPUT] Name http Match * Host localhost Port 2021 URI /log/ingest Format json ``` If your HTTP source has SSL and basic authentication enabled, you will need to add the details of `http_User`, `http_Passwd`, `tls.crt_file`, and `tls.key_file` to the `fluent-bit.conf` file, as shown in the following example. ### Example: Fluent Bit file with SSL and basic authentication enabled The following is an example `fluent-bit.conf` file with SSL and basic authentication enabled on the HTTP source: ``` [INPUT] name tail refresh_interval 5 path test.log read_from_head true [OUTPUT] Name http Match * Host localhost http_User myuser http_Passwd mys3cret tls On tls.crt_file /full/path/to/certfile.crt tls.key_file /full/path/to/keyfile.key Port 2021 URI /log/ingest Format json ``` # Next steps See the [Data Prepper Log Ingestion Demo Guide](https://github.com/opensearch-project/data-prepper/blob/main/examples/log-ingestion/README.md) for a specific example of Apache log ingestion from `FluentBit -> Data Prepper -> OpenSearch` running through Docker. In the future, Data Prepper will offer additional sources and processors that will make more complex log analytics pipelines available. Check out the [Data Prepper Project Roadmap](https://github.com/opensearch-project/data-prepper/projects/1) to see what is coming. If there is a specific source, processor, or sink that you would like to include in your log analytics workflow and is not currently on the roadmap, please bring it to our attention by creating a GitHub issue. Additionally, if you are interested in contributing to Data Prepper, see our [Contributing Guidelines](https://github.com/opensearch-project/data-prepper/blob/main/CONTRIBUTING.md) as well as our [developer guide](https://github.com/opensearch-project/data-prepper/blob/main/docs/developer_guide.md) and [plugin development guide](https://github.com/opensearch-project/data-prepper/blob/main/docs/plugin_development.md). ",
    "url": "http://localhost:4000/data-prepper/docs/latest/common-use-cases/log-analytics/",
    "relUrl": "/common-use-cases/log-analytics/"
  },"25": {
    "doc": "lowercase_string",
    "title": "lowercase_string",
    "content": "# lowercase_string The `lowercase_string` processor converts a string to its lowercase counterpart and is a [mutate string](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/mutate-string-processors#mutate-string-processors) processor. The following table describes options for configuring the `lowercase_string` processor to convert strings to a lowercase format. Option | Required | Type | Description :--- | :--- | :--- | :--- with_keys | Yes | List | A list of keys to convert to lowercase. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/lowercase-string/",
    "relUrl": "/pipelines/configuration/processors/lowercase-string/"
  },"26": {
    "doc": "Managing Data Prepper",
    "title": "Managing Data Prepper",
    "content": "# Managing Data Prepper You can perform administrator functions for Data Prepper, including system configuration, interacting with core APIs, Log4j configuration, and monitoring. You can set up peer forwarding to coordinate multiple Data Prepper nodes when using stateful aggregation. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/managing-data-prepper/managing-data-prepper/",
    "relUrl": "/managing-data-prepper/managing-data-prepper/"
  },"27": {
    "doc": "Migrating from Open Distro",
    "title": "Migrating from Open Distro",
    "content": "# Migrating from Open Distro Existing users can migrate from the Open Distro Data Prepper to OpenSearch Data Prepper. Beginning with Data Prepper version 1.1, there is only one distribution of OpenSearch Data Prepper. ## Change your pipeline configuration The `elasticsearch` sink has changed to `opensearch`. Therefore, change your existing pipeline to use the `opensearch` plugin instead of `elasticsearch`. While the Data Prepper plugin is titled `opensearch`, it remains compatible with Open Distro and ElasticSearch 7.x. {: .note} ## Update Docker image In your Data Prepper Docker configuration, adjust `amazon/opendistro-for-elasticsearch-data-prepper` to `opensearchproject/data-prepper`. This change will download the latest Data Prepper Docker image. ## Next steps For more information about Data Prepper configurations, see [Getting Started with Data Prepper]({{site.url}}{{site.baseurl}}/clients/data-prepper/get-started/). ",
    "url": "http://localhost:4000/data-prepper/docs/latest/migrate-open-distro/",
    "relUrl": "/migrate-open-distro/"
  },"28": {
    "doc": "Migrating from Logstash",
    "title": "Migrating from Logstash",
    "content": "# Migrating from Logstash You can run Data Prepper with a Logstash configuration. As mentioned in [Getting started with Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/), you'll need to configure Data Prepper with a pipeline using a `pipelines.yaml` file. Alternatively, if you have a Logstash configuration `logstash.conf` to configure Data Prepper instead of `pipelines.yaml`. ## Supported plugins As of the Data Prepper 1.2 release, the following plugins from the Logstash configuration are supported: * HTTP Input plugin * Grok Filter plugin * Elasticsearch Output plugin * Amazon Elasticsearch Output plugin ## Limitations * Apart from the supported plugins, all other plugins from the Logstash configuration will throw an `Exception` and fail to run. * Conditionals in the Logstash configuration are not supported as of the Data Prepper 1.2 release. ## Running Data Prepper with a Logstash configuration 1. To install Data Prepper's Docker image, see Installing Data Prepper in [Getting Started with Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started#1-installing-data-prepper). 2. Run the Docker image installed in Step 1 by supplying your `logstash.conf` configuration. ``` docker run --name data-prepper -p 4900:4900 -v ${PWD}/logstash.conf:/usr/share/data-prepper/pipelines.conf opensearchproject/data-prepper:latest pipelines.conf ``` The `logstash.conf` file is converted to `logstash.yaml` by mapping the plugins and attributes in the Logstash configuration to the corresponding plugins and attributes in Data Prepper. You can find the converted `logstash.yaml` file in the same directory where you stored `logstash.conf`. The following output in your terminal indicates that Data Prepper is running correctly: ``` INFO org.opensearch.dataprepper.pipeline.ProcessWorker - log-pipeline Worker: No records received from buffer ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/migrating-from-logstash-data-prepper/",
    "relUrl": "/migrating-from-logstash-data-prepper/"
  },"29": {
    "doc": "Monitoring",
    "title": "Monitoring",
    "content": "# Monitoring Data Prepper with metrics You can monitor Data Prepper with metrics using [Micrometer](https://micrometer.io/). There are two types of metrics: JVM/system metrics and plugin metrics. [Prometheus](https://prometheus.io/) is used as the default metrics backend. ## JVM and system metrics JVM and system metrics are runtime metrics that are used to monitor Data Prepper instances. They include metrics for classloaders, memory, garbage collection, threads, and others. For more information, see [JVM and system metrics](https://micrometer.io/?/docs/ref/jvm). ### Naming JVM and system metrics follow predefined names in [Micrometer](https://micrometer.io/?/docs/concepts#_naming_meters). For example, the Micrometer metrics name for memory usage is `jvm.memory.used`. Micrometer changes the name to match the metrics system. Following the same example, `jvm.memory.used` is reported to Prometheus as `jvm_memory_used`, and is reported to Amazon CloudWatch as `jvm.memory.used.value`. ### Serving By default, metrics are served from the **/metrics/sys** endpoint on the Data Prepper server in Prometheus scrape format. You can configure Prometheus to scrape from the Data Prepper URL. Prometheus then polls Data Prepper for metrics and stores them in its database. To visualize the data, you can set up any frontend that accepts Prometheus metrics, such as [Grafana](https://prometheus.io/docs/visualization/grafana/). You can update the configuration to serve metrics to other registries like Amazon CloudWatch, which does not require or host the endpoint but publishes the metrics directly to CloudWatch. ## Plugin metrics Plugins report their own metrics. Data Prepper uses a naming convention to help with consistency in the metrics. Plugin metrics do not use dimensions. 1. AbstractBuffer - Counter - `recordsWritten`: The number of records written into a buffer - `recordsRead`: The number of records read from a buffer - `recordsProcessed`: The number of records read from a buffer and marked as processed - `writeTimeouts`: The count of write timeouts in a buffer - Gaugefir - `recordsInBuffer`: The number of records in a buffer - `recordsInFlight`: The number of records read from a buffer and being processed by data-prepper downstreams (for example, processor, sink) - Timer - `readTimeElapsed`: The time elapsed while reading from a buffer - `checkpointTimeElapsed`: The time elapsed while checkpointing 2. AbstractProcessor - Counter - `recordsIn`: The number of records ingressed into a processor - `recordsOut`: The number of records egressed from a processor - Timer - `timeElapsed`: The time elapsed during initiation of a processor 3. AbstractSink - Counter - `recordsIn`: The number of records ingressed into a sink - Timer - `timeElapsed`: The time elapsed during execution of a sink ### Naming Metrics follow a naming convention of **PIPELINE_NAME_PLUGIN_NAME_METRIC_NAME**. For example, a **recordsIn** metric for the **opensearch-sink** plugin in a pipeline named **output-pipeline** has a qualified name of **output-pipeline_opensearch_sink_recordsIn**. ### Serving By default, metrics are served from the **/metrics/sys** endpoint on the Data Prepper server in a Prometheus scrape format. You can configure Prometheus to scrape from the Data Prepper URL. The Data Prepper server port has a default value of `4900` that you can modify, and this port can be used for any frontend that accepts Prometheus metrics, such as [Grafana](https://prometheus.io/docs/visualization/grafana/). You can update the configuration to serve metrics to other registries like CloudWatch, that does not require or host the endpoint, but publishes the metrics directly to CloudWatch. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/managing-data-prepper/monitoring/",
    "relUrl": "/managing-data-prepper/monitoring/"
  },"30": {
    "doc": "Mutate event",
    "title": "Mutate event",
    "content": "# Mutate event processors Mutate event processors allow you to modify events in Data Prepper. The following processors are available: * [add_entries]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/add-entries/) allows you to add entries to an event. * [copy_values]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/copy-values/) allows you to copy values within an event. * [delete_entries]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/delete-entries/) allows you to delete entries from an event. * [rename_keys]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/rename-keys/) allows you to rename keys in an event. * [convert_entry_type]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/convert_entry_type/) allows you to convert value types in an event. * [list_to_map]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/list-to-map) allows you to convert list of objects from an event where each object contains a `key` field into a map of target keys. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/mutate-event/",
    "relUrl": "/pipelines/configuration/processors/mutate-event/"
  },"31": {
    "doc": "Mutate string",
    "title": "Mutate string",
    "content": "# Mutate string processors You can change the way that a string appears by using a mutate string processesor. For example, you can use the `uppercase_string` processor to convert a string to uppercase, and you can use the `lowercase_string` processor to convert a string to lowercase. The following is a list of processors that allow you to mutate a string: * [substitute_string](#substitute_string) * [split_string](#split_string) * [uppercase_string](#uppercase_string) * [lowercase_string](#lowercase_string) * [trim_string](#trim_string) ## substitute_string The `substitute_string` processor matches a key's value against a regular expression (regex) and replaces all returned matches with a replacement string. ### Configuration You can configure the `substitute_string` processor with the following options. Option | Required | Description :--- | :--- | :--- `entries` | Yes | A list of entries to add to an event. | `source` | Yes | The key to be modified. | `from` | Yes | The regex string to be replaced. Special regex characters such as `[` and `]` must be escaped using `\\\\` when using double quotes and `\\` when using single quotes. For more information, see [Class Pattern](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/regex/Pattern.html) in the Java documentation. | `to` | Yes | The string that replaces each match of `from`. | ### Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - substitute_string: entries: - source: \"message\" from: \":\" to: \"-\" sink: - stdout: ``` {% include copy.html %} Next, create a log file named `logs_json.log`. After that, replace the `path` of the file source in your `pipeline.yaml` file with your file path. For more detailed information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). Before you run Data Prepper, the source appears in the following format: ```json {\"message\": \"ab:cd:ab:cd\"} ``` After you run Data Prepper, the source is converted to the following format: ```json {\"message\": \"ab-cd-ab-cd\"} ``` `from` defines which string is replaced, and `to` defines the string that replaces the `from` string. In the preceding example, string `ab:cd:ab:cd` becomes `ab-cd-ab-cd`. If the `from` regex string does not return a match, the key is returned without any changes. ## split_string The `split_string` processor splits a field into an array using a delimiter character. ### Configuration You can configure the `split_string` processor with the following options. Option | Required | Description :--- | :--- | :--- `entries` | Yes | A list of entries to add to an event. | `source` | Yes | The key to be split. | `delimiter` | No | The separator character responsible for the split. Cannot be defined at the same time as `delimiter_regex`. At least `delimiter` or `delimiter_regex` must be defined. | `delimiter_regex` | No | A regex string responsible for the split. Cannot be defined at the same time as `delimiter`. Either `delimiter` or `delimiter_regex` must be defined. | ### Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - split_string: entries: - source: \"message\" delimiter: \",\" sink: - stdout: ``` {% include copy.html %} Next, create a log file named `logs_json.log`. After that, replace the `path` in the file source of your `pipeline.yaml` file with your file path. For more detailed information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). Before you run Data Prepper, the source appears in the following format: ```json {\"message\": \"hello,world\"} ``` After you run Data Prepper, the source is converted to the following format: ```json {\"message\":[\"hello\",\"world\"]} ``` ## uppercase_string The `uppercase_string` processor converts the value (a string) of a key from its current case to uppercase. ### Configuration You can configure the `uppercase_string` processor with the following options. Option | Required | Description :--- | :--- | :--- `with_keys` | Yes | A list of keys to convert to uppercase. | ### Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - uppercase_string: with_keys: - \"uppercaseField\" sink: - stdout: ``` {% include copy.html %} Next, create a log file named `logs_json.log`. After that, replace the `path` in the file source of your `pipeline.yaml` file with the correct file path. For more detailed information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). Before you run Data Prepper, the source appears in the following format: ```json {\"uppercaseField\": \"hello\"} ``` After you run Data Prepper, the source is converted to the following format: ```json {\"uppercaseField\": \"HELLO\"} ``` ## lowercase_string The `lowercase string` processor converts a string to lowercase. ### Configuration You can configure the `lowercase string` processor with the following options. Option | Required | Description :--- | :--- | :--- `with_keys` | Yes | A list of keys to convert to lowercase. | ### Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - lowercase_string: with_keys: - \"lowercaseField\" sink: - stdout: ``` {% include copy.html %} Next, create a log file named `logs_json.log`. After that, replace the `path` in the file source of your `pipeline.yaml` file with the correct file path. For more detailed information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). Before you run Data Prepper, the source appears in the following format: ```json {\"lowercaseField\": \"TESTmeSSage\"} ``` After you run Data Prepper, the source is converted to the following format: ```json {\"lowercaseField\": \"testmessage\"} ``` ## trim_string The `trim_string` processor removes whitespace from the beginning and end of a key. ### Configuration You can configure the `trim_string` processor with the following options. Option | Required | Description :--- | :--- | :--- `with_keys` | Yes | A list of keys from which to trim the whitespace. | ### Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - trim_string: with_keys: - \"trimField\" sink: - stdout: ``` {% include copy.html %} Next, create a log file named `logs_json.log`. After that, replace the `path` in the file source of your `pipeline.yaml` file with the correct file path. For more detailed information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). Before you run Data Prepper, the source appears in the following format: ```json {\"trimField\": \" Space Ship \"} ``` After you run Data Prepper, the source is converted to the following format: ```json {\"trimField\": \"Space Ship\"} ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/mutate-string/",
    "relUrl": "/pipelines/configuration/processors/mutate-string/"
  },"32": {
    "doc": "OpenSearch sink",
    "title": "OpenSearch sink",
    "content": "# OpenSearch sink You can use the `opensearch` sink plugin to send data to an OpenSearch cluster, a legacy Elasticsearch cluster, or an Amazon OpenSearch Service domain. The plugin supports OpenSearch 1.0 and later and Elasticsearch 7.3 and later. ## Usage To configure an `opensearch` sink, specify the `opensearch` option within the pipeline configuration: ```yaml pipeline: ... sink: opensearch: hosts: [\"https://localhost:9200\"] cert: path/to/cert username: YOUR_USERNAME password: YOUR_PASSWORD index_type: trace-analytics-raw dlq_file: /your/local/dlq-file max_retries: 20 bulk_size: 4 ``` To configure an [Amazon OpenSearch Service](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html) sink, specify the domain endpoint as the `hosts` option: ```yaml pipeline: ... sink: opensearch: hosts: [\"https://your-amazon-opensearch-service-endpoint\"] aws_sigv4: true cert: path/to/cert insecure: false index_type: trace-analytics-service-map bulk_size: 4 ``` ## Configuration options The following table describes options you can configure for the `opensearch` sink. Option | Required | Type | Description :--- | :--- | :--- | :--- hosts | Yes | List | List of OpenSearch hosts to write to (for example, `[\"https://localhost:9200\", \"https://remote-cluster:9200\"]`). cert | No | String | Path to the security certificate (for example, `\"config/root-ca.pem\"`) if the cluster uses the OpenSearch Security plugin. username | No | String | Username for HTTP basic authentication. password | No | String | Password for HTTP basic authentication. aws_sigv4 | No | Boolean | Default value is false. Whether to use AWS Identity and Access Management (IAM) signing to connect to an Amazon OpenSearch Service domain. For your access key, secret key, and optional session token, Data Prepper uses the default credential chain (environment variables, Java system properties, `~/.aws/credential`, etc.). aws_region | No | String | The AWS region (for example, `\"us-east-1\"`) for the domain if you are connecting to Amazon OpenSearch Service. aws_sts_role_arn | No | String | IAM role that the plugin uses to sign requests sent to Amazon OpenSearch Service. If this information is not provided, the plugin uses the default credentials. [max_retries](#configure-max_retries) | No | Integer | The maximum number of times the OpenSearch sink should try to push data to the OpenSearch server before considering it to be a failure. Defaults to `Integer.MAX_VALUE`. If not provided, the sink will try to push data to the OpenSearch server indefinitely because the default value is high and exponential backoff would increase the waiting time before retry. socket_timeout | No | Integer | The timeout, in milliseconds, waiting for data to return (or the maximum period of inactivity between two consecutive data packets). A timeout value of zero is interpreted as an infinite timeout. If this timeout value is negative or not set, the underlying Apache HttpClient would rely on operating system settings for managing socket timeouts. connect_timeout | No | Integer | The timeout in milliseconds used when requesting a connection from the connection manager. A timeout value of zero is interpreted as an infinite timeout. If this timeout value is negative or not set, the underlying Apache HttpClient would rely on operating system settings for managing connection timeouts. insecure | No | Boolean | Whether or not to verify SSL certificates. If set to true, certificate authority (CA) certificate verification is disabled and insecure HTTP requests are sent instead. Default value is `false`. proxy | No | String | The address of a [forward HTTP proxy server](https://en.wikipedia.org/wiki/Proxy_server). The format is \"&lt;host name or IP&gt;:&lt;port&gt;\". Examples: \"example.com:8100\", \"http://example.com:8100\", \"112.112.112.112:8100\". Port number cannot be omitted. index | Conditionally | String | Name of the export index. Applicable and required only when the `index_type` is `custom`. index_type | No | String | This index type tells the Sink plugin what type of data it is handling. Valid values: `custom`, `trace-analytics-raw`, `trace-analytics-service-map`, `management-disabled`. Default value is `custom`. template_file | No | String | Path to a JSON [index template]({{site.url}}{{site.baseurl}}/opensearch/index-templates/) file (for example, `/your/local/template-file.json`) if `index_type` is `custom`. See [otel-v1-apm-span-index-template.json](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-plugins/opensearch/src/main/resources/otel-v1-apm-span-index-template.json) for an example. document_id_field | No | String | The field from the source data to use for the OpenSearch document ID (for example, `\"my-field\"`) if `index_type` is `custom`. dlq_file | No | String | The path to your preferred dead letter queue file (for example, `/your/local/dlq-file`). Data Prepper writes to this file when it fails to index a document on the OpenSearch cluster. dlq | No | N/A | DLQ configurations. See [Dead Letter Queues]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/dlq/) for details. If the `dlq_file` option is also available, the sink will fail. bulk_size | No | Integer (long) | The maximum size (in MiB) of bulk requests sent to the OpenSearch cluster. Values below 0 indicate an unlimited size. If a single document exceeds the maximum bulk request size, Data Prepper sends it individually. Default value is 5. ism_policy_file | No | String | The absolute file path for an ISM (Index State Management) policy JSON file. This policy file is effective only when there is no built-in policy file for the index type. For example, `custom` index type is currently the only one without a built-in policy file, thus it would use the policy file here if it's provided through this parameter. For more information, see [ISM policies]({{site.url}}{{site.baseurl}}/im-plugin/ism/policies/). number_of_shards | No | Integer | The number of primary shards that an index should have on the destination OpenSearch server. This parameter is effective only when `template_file` is either explicitly provided in Sink configuration or built-in. If this parameter is set, it would override the value in index template file. For more information, see [Create index]({{site.url}}{{site.baseurl}}/api-reference/index-apis/create-index/). number_of_replicas | No | Integer | The number of replica shards each primary shard should have on the destination OpenSearch server. For example, if you have 4 primary shards and set number_of_replicas to 3, the index has 12 replica shards. This parameter is effective only when `template_file` is either explicitly provided in Sink configuration or built-in. If this parameter is set, it would override the value in index template file. For more information, see [Create index]({{site.url}}{{site.baseurl}}/api-reference/index-apis/create-index/). ### Configure max_retries You can include the `max_retries` option in your pipeline configuration to control the number of times the source tries to write to sinks with exponential backoff. If you don't include this option, pipelines keep retrying forever. If you specify `max_retries` and a pipeline has a [dead-letter queue (DLQ)]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/dlq/) configured, the pipeline will keep trying to write to sinks until it reaches the maximum number of retries, at which point it starts to send failed data to the DLQ. If you don't specify `max_retries`, only data that is rejected by sinks is written to the DLQ. Pipelines continue to try to write all other data to the sinks. ## OpenSearch cluster security In order to send data to an OpenSearch cluster using the `opensearch` sink plugin, you must specify your username and password within the pipeline configuration. The following example `pipelines.yaml` file demonstrates how to specify admin security credentials: ```yaml sink: - opensearch: username: \"admin\" password: \"admin\" ... ``` Alternately, rather than admin credentials, you can specify the credentials of a user mapped to a role with the minimum permissions listed in the following sections. ### Cluster permissions - `cluster_all` - `indices:admin/template/get` - `indices:admin/template/put` ### Index permissions - Index: `otel-v1*`; Index permission: `indices_all` - Index: `.opendistro-ism-config`; Index permission: `indices_all` - Index: `*`; Index permission: `manage_aliases` For instructions on how to map users to roles, see [Map users to roles]({{site.url}}{{site.baseurl}}/security/access-control/users-roles/#map-users-to-roles). ## Amazon OpenSearch Service domain security The `opensearch` sink plugin can send data to an [Amazon OpenSearch Service](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html) domain, which uses IAM for security. The plugin uses the default credential chain. Run `aws configure` using the [AWS Command Line Interface (AWS CLI)](https://aws.amazon.com/cli/) to set your credentials. Make sure the credentials that you configure have the required IAM permissions. The following domain access policy demonstrates the minimum required permissions: ```json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam:::user/data-prepper-user\" }, \"Action\": \"es:ESHttp*\", \"Resource\": [ \"arn:aws:es:us-east-1::domain//otel-v1*\", \"arn:aws:es:us-east-1::domain//_template/otel-v1*\", \"arn:aws:es:us-east-1::domain//_plugins/_ism/policies/raw-span-policy\", \"arn:aws:es:us-east-1::domain//_alias/otel-v1*\", \"arn:aws:es:us-east-1::domain//_alias/_bulk\" ] }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam:::user/data-prepper-user\" }, \"Action\": \"es:ESHttpGet\", \"Resource\": \"arn:aws:es:us-east-1::domain//_cluster/settings\" } ] } ``` For instructions on how to configure the domain access policy, see [Resource-based policies ](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ac.html#ac-types-resource) in the Amazon OpenSearch Service documentation. ### Fine-grained access control If your OpenSearch Service domain uses [fine-grained access control](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/fgac.html), the `opensearch` sink plugin requires some additional configuration. #### IAM ARN as master user If you're using an IAM Amazon Resource Name (ARN) as the master user, include the `aws_sigv4` option in your sink configuration: ```yaml ... sink: opensearch: hosts: [\"https://your-fgac-amazon-opensearch-service-endpoint\"] aws_sigv4: true ``` Run `aws configure` using the AWS CLI to use the master IAM user credentials. If you don't want to use the master user, you can specify a different IAM role using the `aws_sts_role_arn` option. The plugin will then use this role to sign requests sent to the domain sink. The ARN that you specify must be included in the [domain access policy]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sinks/opensearch/#amazon-opensearch-service-domain-security). #### Master user in the internal user database If your domain uses a master user in the internal user database, specify the master username and password as well as the `aws_sigv4` option: ```yaml sink: opensearch: hosts: [\"https://your-fgac-amazon-opensearch-service-endpoint\"] aws_sigv4: false username: \"master-username\" password: \"master-password\" ``` For more information, see [Recommended configurations](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/fgac.html#fgac-recommendations) in the Amazon OpenSearch Service documentation. ***Note***: You can create a new IAM role or internal user database user with the `all_access` permission and use it instead of the master user. ## OpenSearch Serverless collection security The `opensearch` sink plugin can send data to an [Amazon OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html) collection. OpenSearch Serverless collection sinks have the following limitations: - You can't write to a collection that uses virtual private cloud (VPC) access. The collection must be accessible from public networks. - The OTel trace group processor doesn't currently support collection sinks. ### Creating a pipeline role First, create an IAM role that the pipeline will assume in order to write to the collection. The role must have the following minimum permissions: ```json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"aoss:BatchGetCollection\" ], \"Resource\": \"*\" } ] } ``` The role must have the following trust relationship, which allows the pipeline to assume it: ```json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam:::root\" }, \"Action\": \"sts:AssumeRole\" } ] } ``` ### Creating a collection Next, create a collection with the following settings: - Public [network access](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-network.html) to both the OpenSearch endpoint and OpenSearch Dashboards. - The following [data access policy](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-data-access.html), which grants the required permissions to the pipeline role: ```json [ { \"Rules\":[ { \"Resource\":[ \"index/collection-name/*\" ], \"Permission\":[ \"aoss:CreateIndex\", \"aoss:UpdateIndex\", \"aoss:DescribeIndex\", \"aoss:WriteDocument\" ], \"ResourceType\":\"index\" } ], \"Principal\":[ \"arn:aws:iam:::role/PipelineRole\" ], \"Description\":\"Pipeline role access\" } ] ``` ***Important***: Make sure to replace the ARN in the `Principal` element with the ARN of the pipeline role that you created in the preceding step. For instructions on how to create collections, see [Creating collections](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-manage.html#serverless-create) in the Amazon OpenSearch Service documentation. ### Creating a pipeline Within your `pipelines.yaml` file, specify the OpenSearch Serverless collection endpoint as the `hosts` option. In addition, you must set the `serverless` option to `true`. Specify the pipeline role in the `sts_role_arn` option: ```yaml log-pipeline: source: http: processor: - date: from_time_received: true destination: \"@timestamp\" sink: - opensearch: hosts: [ \"https://\" ] index: \"my-serverless-index\" aws: serverless: true sts_role_arn: \"arn:aws:iam:::role/PipelineRole\" region: \"us-east-1\" ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sinks/opensearch/",
    "relUrl": "/pipelines/configuration/sinks/opensearch/"
  },"33": {
    "doc": "otel_logs_source",
    "title": "otel_logs_source",
    "content": "# otel_logs_source The `otel_logs_source` source is an OpenTelemetry source that follows the [OpenTelemetry Protocol Specification](https://github.com/open-telemetry/oteps/blob/master/text/0035-opentelemetry-protocol.md) and receives logs from the OTel Collector in the form of `ExportLogsServiceRequest` records. This source supports the `OTLP/gRPC` protocol. {: .note} ## Configuration You can configure the `otel_logs_source` source with the following options. | Option | Type | Description | :--- | :--- | :--- | port | int | Represents the port that the `otel_logs_source` source is running on. Default value is `21892`. | path | string | Represents the path for sending unframed HTTP requests. You can use this option to support an unframed gRPC request with an HTTP idiomatic path to a configurable path. The path should start with `/`, and its length should be at least 1. The `/opentelemetry.proto.collector.logs.v1.LogsService/Export` endpoint is disabled for both gRPC and HTTP requests if the path is configured. The path can contain a `${pipelineName}` placeholder, which is replaced with the pipeline name. If the value is empty and `unframed_requests` is `true`, then the path that the source provides is `/opentelemetry.proto.collector.logs.v1.LogsService/Export`. | request_timeout | int | Represents the request timeout duration in milliseconds. Default value is `10000`. | health_check_service | Boolean | Enables the gRPC health check service under `grpc.health.v1/Health/Check`. Default value is `false`. | proto_reflection_service | Boolean | Enables a reflection service for Protobuf services (see [ProtoReflectionService](https://grpc.github.io/grpc-java/javadoc/io/grpc/protobuf/services/ProtoReflectionService.html) and [gRPC reflection](https://github.com/grpc/grpc-java/blob/master/documentation/server-reflection-tutorial.md)). Default value is `false`. | unframed_requests | Boolean | Enables requests that are not framed using the gRPC wire protocol. Default value is `false`. | thread_count | int | The number of threads to keep in the `ScheduledThreadPool`. Default value is `500`. | max_connection_count | int | The maximum number of open connections allowed. Default value is `500`. | ### SSL You can configure SSL in the `otel_logs_source` source with the following options. | Option | Type | Description | :--- | :--- | :--- | ssl | Boolean | Enables TLS/SSL. Default value is `true`. | sslKeyCertChainFile | string | Represents the SSL certificate chain file path or Amazon Simple Storage Service (Amazon S3) path. For example, see the Amazon S3 path `s3:///`. Required if `ssl` is set to `true`. | sslKeyFile | string | Represents the SSL key file path or Amazon S3 path. For example, see the Amazon S3 path `s3:///`. Required if `ssl` is set to `true`. | useAcmCertForSSL | Boolean | Enables TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is `false`. | acmCertificateArn | string | Represents the ACM certificate Amazon Resource Name (ARN). ACM certificates take precedence over Amazon S3 or local file system certificates. Required if `useAcmCertForSSL` is set to `true`. | awsRegion | string | Represents the AWS Region used by ACM or Amazon S3. Required if `useAcmCertForSSL` is set to `true` or `sslKeyCertChainFile` or `sslKeyFile` is the Amazon S3 path. | ## Usage To get started, create a `pipeline.yaml` file and add `otel_logs_source` as the source: ``` source: - otel_logs_source: ``` ## Metrics You can use the following metrics with the `otel_logs_source` source. | Option | Type | Description | :--- | :--- | :--- | `requestTimeouts` | Counter | Measures the total number of requests that time out. | `requestsReceived` | Counter | Measures the total number of requests received by the `otel_logs_source` source. | `badRequests` | Counter | Measures the total number of requests that could not be parsed. | `requestsTooLarge` | Counter | Measures the total number of requests that exceed the maximum allowed size. Indicates that the size of the data being written into the buffer is beyond the buffer's maximum capacity. | `internalServerError` | Counter | Measures the total number of requests that are erroneous due to errors other than `requestTimeouts` or `requestsTooLarge`. | `successRequests` | Counter | Measures the total number of requests successfully written to the buffer. | `payloadSize` | Distribution summary | Measures the distribution of all incoming payload sizes. | `requestProcessDuration` | Timer | Measures the duration of request processing. | ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sources/otel-logs-source/",
    "relUrl": "/pipelines/configuration/sources/otel-logs-source/"
  },"34": {
    "doc": "otel_metrics_source",
    "title": "otel_metrics_source",
    "content": "# otel_metrics_source `otel_metrics_source` is an OpenTelemetry Collector source that collects metric data. The following table describes options you can use to configure the `otel_metrics_source` source. Option | Required | Type | Description :--- | :--- | :--- | :--- port | No | Integer | The port that the OpenTelemtry metrics source runs on. Default value is `21891`. request_timeout | No | Integer | The request timeout, in milliseconds. Default value is `10000`. health_check_service | No | Boolean | Enables a gRPC health check service under `grpc.health.v1/Health/Check`. Default value is `false`. proto_reflection_service | No | Boolean | Enables a reflection service for Protobuf services (see [gRPC reflection](https://github.com/grpc/grpc/blob/master/doc/server-reflection.md) and [gRPC Server Reflection Tutorial](https://github.com/grpc/grpc-java/blob/master/documentation/server-reflection-tutorial.md) docs). Default value is `false`. unframed_requests | No | Boolean | Enables requests not framed using the gRPC wire protocol. thread_count | No | Integer | The number of threads to keep in the `ScheduledThreadPool`. Default value is `200`. max_connection_count | No | Integer | The maximum allowed number of open connections. Default value is `500`. ssl | No | Boolean | Enables connections to the OpenTelemetry source port over TLS/SSL. Default value is `true`. sslKeyCertChainFile | Conditionally | String | File-system path or Amazon Simple Storage Service (Amazon S3) path to the security certificate (for example, `\"config/demo-data-prepper.crt\"` or `\"s3://my-secrets-bucket/demo-data-prepper.crt\"`). Required if `ssl` is set to `true`. sslKeyFile | Conditionally | String | File-system path or Amazon S3 path to the security key (for example, `\"config/demo-data-prepper.key\"` or `\"s3://my-secrets-bucket/demo-data-prepper.key\"`). Required if `ssl` is set to `true`. useAcmCertForSSL | No | Boolean | Whether to enable TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is `false`. acmCertificateArn | Conditionally | String | Represents the ACM certificate ARN. ACM certificate take preference over S3 or local file system certificates. Required if `useAcmCertForSSL` is set to `true`. awsRegion | Conditionally | String | Represents the AWS Region used by ACM or Amazon S3. Required if `useAcmCertForSSL` is set to `true` or `sslKeyCertChainFile` and `sslKeyFile` is the Amazon S3 path. authentication | No | Object | An authentication configuration. By default, an unauthenticated server is created for the pipeline. This uses pluggable authentication for HTTPS. To use basic authentication, define the `http_basic` plugin with a `username` and `password`. To provide customer authentication, use or create a plugin that implements [GrpcAuthenticationProvider](https://github.com/opensearch-project/data-prepper/blob/1.2.0/data-prepper-plugins/armeria-common/src/main/java/com/amazon/dataprepper/armeria/authentication/GrpcAuthenticationProvider.java). ## Metrics The `otel_metrics_source` source includes the following metrics. ### Counters - `requestTimeouts`: Measures the total number of requests that time out. - `requestsReceived`: Measures the total number of requests received by the OpenTelemetry metrics source. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sources/otel-metrics-source/",
    "relUrl": "/pipelines/configuration/sources/otel-metrics-source/"
  },"35": {
    "doc": "otel_metrics",
    "title": "otel_metrics",
    "content": "# otel_metrics The `otel_metrics` processor serializes a collection of `ExportMetricsServiceRequest` records sent from the [OTel metrics source]({{site.url}}{{site.baseurl}}//data-prepper/pipelines/configuration/sources/otel-metrics-source/) into a collection of string records. ## Usage To get started, add the following processor to your `pipeline.yaml` configuration file: ``` yaml processor: - otel_metrics_raw_processor: ``` {% include copy.html %} ## Configuration You can use the following optional parameters to configure histogram buckets and their default values. A histogram displays numerical data by grouping data into buckets. You can use histogram buckets to view sets of events that are organized by the total event count and aggregate sum for all events. For more detailed information, see [OpenTelemetry Histograms](https://opentelemetry.io/docs/reference/specification/metrics/data-model/#histogram). | Parameter | Default value | Description | :--- | :--- | :--- | `calculate_histogram_buckets` | `True` | Whether or not to calculate histogram buckets. | `calculate_exponential_histogram_buckets` | `True` | Whether or not to calculate exponential histogram buckets. | `exponential_histogram_max_allowed_scale` | `10` | Maximum allowed scale in exponential histogram calculation. | `flatten_attributes` | `False` | Whether or not to flatten the `attributes` field in the JSON data. | ### calculate_histogram_buckets If `calculate_histogram_buckets` is not set to `false`, then the following `JSON` file will be added to every histogram JSON. If `flatten_attributes` is set to `false`, the `JSON` string format of the metrics does not change the attributes field. If `flatten_attributes` is set to `true`, the values in the attributes field are placed in the parent `JSON` object. The default value is `true`. See the following `JSON` example: ```json \"buckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 } ] ``` You can create detailed representations of histogram buckets and their boundaries. You can control this feature by using the following parameters in your `pipeline.yaml` file: ```yaml processor: - otel_metrics_raw_processor: calculate_histogram_buckets: true calculate_exponential_histogram_buckets: true exponential_histogram_max_allowed_scale: 10 flatten_attributes: false ``` {% include copy.html %} Each array element describes one bucket. Each bucket contains the lower boundary, upper boundary, and its value count. This is a specific form of more detailed OpenTelemetry representation that is a part of the `JSON` output created by the `otel_metrics` processor. See the following `JSON` file, which is added to each `JSON` histogram by the `otel_metrics` processor: ```json \"explicitBounds\": [ 5.0, 10.0 ], \"bucketCountsList\": [ 2, 5 ] ``` ### calculate_exponential_histogram_buckets If `calculate_exponential_histogram_buckets` is set to `true` (the default setting), the following `JSON` values are added to each `JSON` histogram: ```json \"negativeBuckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 } ], ... \"positiveBuckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 } ], ``` The following `JSON` file is a more detailed form of OpenTelemetry representation that consists of negative and positive buckets, a scale parameter, an offset, and a list of bucket counts: ```json \"negative\": [ 1, 2, 3 ], \"positive\": [ 1, 2, 3 ], \"scale\" : -3, \"negativeOffset\" : 0, \"positiveOffset\" : 1 ``` ### exponential_histogram_max_allowed_scale The `exponential_histogram_max_allowed_scale` parameter defines the maximum allowed scale for an exponential histogram. If you increase this parameter, you will increase potential memory consumption. See the [OpenTelemetry specifications](https://github.com/open-telemetry/opentelemetry-proto/blob/main/opentelemetry/proto/metrics/v1/metrics.proto) for more information on exponential histograms and their computational complexity. All exponential histograms that have a scale that is above the configured parameter (by default, a value of `10`) are discarded and logged with an error level. You can check the log that Data Prepper creates to see the `ERROR` log message. The absolute scale value is used for comparison, so a scale of `-11` that is treated equally to `11` exceeds the configured value of `10` and can be discarded. {: .note} ## Metrics The following table describes metrics that are common to all processors. | Metric name | Type | Description | ------------- | ---- | -----------| `recordsIn` | Counter | Metric representing the number of ingress records. | `recordsOut` | Counter | Metric representing the number of egress records. | `timeElapsed` | Timer | Metric representing the time elapsed during execution of records. | ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/otel-metrics/",
    "relUrl": "/pipelines/configuration/processors/otel-metrics/"
  },"36": {
    "doc": "otel_trace_group",
    "title": "otel_trace_group",
    "content": "# otel_trace_group The `otel_trace_group` processor completes missing trace-group-related fields in the collection of [span](https://github.com/opensearch-project/data-prepper/blob/834f28fdf1df6d42a6666e91e6407474b88e7ec6/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/trace/Span.java) records by looking up the OpenSearch backend. The `otel_trace_group` processor identifies the missing trace group information for a `spanId` by looking up the relevant fields in its root `span` stored in OpenSearch. ## OpenSearch When you connect to an OpenSearch cluster using your username and password, use the following example `pipeline.yaml` file to configure the `otel_trace_group` processor: ``` YAML pipeline: ... processor: - otel_trace_group: hosts: [\"https://localhost:9200\"] cert: path/to/cert username: YOUR_USERNAME_HERE password: YOUR_PASSWORD_HERE ``` See [OpenSearch security]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sinks/opensearch/#opensearch-cluster-security) for a more detailed explanation of which OpenSearch credentials and permissions are required and how to configure those credentials for the OTel trace group processor. ### Amazon OpenSearch Service When you use [Amazon OpenSearch Service]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sinks/opensearch/#amazon-opensearch-service-domain-security), use the following example `pipeline.yaml` file to configure the `otel_trace_group` processor: ``` YAML pipeline: ... processor: - otel_trace_group: hosts: [\"https://your-amazon-opensearch-service-endpoint\"] aws_sigv4: true cert: path/to/cert insecure: false ``` ## Configuration You can configure the `otel_trace_group` processor with the following options. | Name | Description | Default value | -----| ----| -----------| `hosts`| A list of IP addresses of OpenSearch nodes. Required. | No default value. | `cert` | A certificate authority (CA) certificate that is PEM encoded. Accepts both .pem or .crt. This enables the client to trust the CA that has signed the certificate that OpenSearch is using. | `null` | `aws_sigv4` | A Boolean flag used to sign the HTTP request with AWS credentials. Only applies to Amazon OpenSearch Service. See [OpenSearch security](https://github.com/opensearch-project/data-prepper/blob/129524227779ee35a327c27c3098d550d7256df1/data-prepper-plugins/opensearch/security.md) for details. | `false`. | `aws_region` | A string that represents the AWS Region of the Amazon OpenSearch Service domain, for example, `us-west-2`. Only applies to Amazon OpenSearch Service. | `us-east-1` | `aws_sts_role_arn`| An AWS Identity and Access Management (IAM) role that the sink plugin assumes to sign the request to Amazon OpenSearch Service. If not provided, the plugin uses the [default credentials](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/auth/credentials/DefaultCredentialsProvider.html). | `null` | `aws_sts_header_overrides` | A map of header overrides that the IAM role assumes for the sink plugin. | `null` | `insecure` | A Boolean flag used to turn off SSL certificate verification. If set to `true`, CA certificate verification is turned off and insecure HTTP requests are sent. | `false` | `username` | A string that contains the username and is used in the [internal users](https://opensearch.org/docs/latest/security/access-control/users-roles/) `YAML` configuration file of your OpenSearch cluster. | `null` | `password` | A string that contains the password and is used in the [internal users](https://opensearch.org/docs/latest/security/access-control/users-roles/) `YAML` configuration file of your OpenSearch cluster. | `null` | ## Configuration option examples You can define the configuration option values in the `aws_sts_header_overrides` option. See the following example: ``` aws_sts_header_overrides: x-my-custom-header-1: my-custom-value-1 x-my-custom-header-2: my-custom-value-2 ``` ## Metrics The following table describes custom metrics specific to the `otel_trace_group` processor. | Metric name | Type | Description | ------------- | ---- | ----------- | `recordsInMissingTraceGroup` | Counter | The number of ingress records missing trace group fields. | `recordsOutFixedTraceGroup` | Counter | The number of egress records with successfully completed trace group fields. | `recordsOutMissingTraceGroup` | Counter | The number of egress records missing trace group fields. | ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/otel-trace-group/",
    "relUrl": "/pipelines/configuration/processors/otel-trace-group/"
  },"37": {
    "doc": "otel_trace",
    "title": "otel_trace",
    "content": "# otel_trace The `otel_trace` processor completes trace-group-related fields in all incoming Data Prepper span records by state caching the root span information for each `tradeId`. ## Parameters This processor includes the following parameters. * `traceGroup`: Root span name * `endTime`: End time of the entire trace in International Organization for Standardization (ISO) 8601 format * `durationInNanos`: Duration of the entire trace in nanoseconds * `statusCode`: Status code for the entire trace in nanoseconds ## Configuration The following table describes the options you can use to configure the `otel_trace` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- trace_flush_interval | No | Integer | Represents the time interval in seconds to flush all the descendant spans without any root span. Default is 180. ## Metrics The following table describes common [Abstract processor](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/processor/AbstractProcessor.java) metrics. | Metric name | Type | Description | ------------- | ---- | -----------| `recordsIn` | Counter | Metric representing the ingress of records to a pipeline component. | `recordsOut` | Counter | Metric representing the egress of records from a pipeline component. | `timeElapsed` | Timer | Metric representing the time elapsed during execution of a pipeline component. | The `otel_trace` processor includes the following custom metrics: * `traceGroupCacheCount`: The number of trace groups in the trace group cache. * `spanSetCount`: The number of span sets in the span set collection. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/otel-trace-raw/",
    "relUrl": "/pipelines/configuration/processors/otel-trace-raw/"
  },"38": {
    "doc": "otel_trace_source source",
    "title": "otel_trace_source source",
    "content": "# otel_trace source ## Overview The `otel_trace` source is a source for the OpenTelemetry Collector. The following table describes options you can use to configure the `otel_trace` source. Option | Required | Type | Description :--- | :--- | :--- | :--- port | No | Integer | The port that the `otel_trace` source runs on. Default value is `21890`. request_timeout | No | Integer | The request timeout, in milliseconds. Default value is `10000`. health_check_service | No | Boolean | Enables a gRPC health check service under `grpc.health.v1/Health/Check`. Default value is `false`. unauthenticated_health_check | No | Boolean | Determines whether or not authentication is required on the health check endpoint. Data Prepper ignores this option if no authentication is defined. Default value is `false`. proto_reflection_service | No | Boolean | Enables a reflection service for Protobuf services (see [gRPC reflection](https://github.com/grpc/grpc/blob/master/doc/server-reflection.md) and [gRPC Server Reflection Tutorial](https://github.com/grpc/grpc-java/blob/master/documentation/server-reflection-tutorial.md) docs). Default value is `false`. unframed_requests | No | Boolean | Enable requests not framed using the gRPC wire protocol. thread_count | No | Integer | The number of threads to keep in the ScheduledThreadPool. Default value is `200`. max_connection_count | No | Integer | The maximum allowed number of open connections. Default value is `500`. ssl | No | Boolean | Enables connections to the OTel source port over TLS/SSL. Defaults to `true`. sslKeyCertChainFile | Conditionally | String | File system path or Amazon Simple Storage Service (Amazon S3) path to the security certificate (for example, `\"config/demo-data-prepper.crt\"` or `\"s3://my-secrets-bucket/demo-data-prepper.crt\"`). Required if `ssl` is set to `true`. sslKeyFile | Conditionally | String | File system path or Amazon S3 path to the security key (for example, `\"config/demo-data-prepper.key\"` or `\"s3://my-secrets-bucket/demo-data-prepper.key\"`). Required if `ssl` is set to `true`. useAcmCertForSSL | No | Boolean | Whether to enable TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is `false`. acmCertificateArn | Conditionally | String | Represents the ACM certificate ARN. ACM certificate take preference over S3 or local file system certificate. Required if `useAcmCertForSSL` is set to `true`. awsRegion | Conditionally | String | Represents the AWS region used by ACM or Amazon S3. Required if `useAcmCertForSSL` is set to `true` or `sslKeyCertChainFile` and `sslKeyFile` are Amazon S3 paths. authentication | No | Object | An authentication configuration. By default, an unauthenticated server is created for the pipeline. This parameter uses pluggable authentication for HTTPS. To use basic authentication, define the `http_basic` plugin with a `username` and `password`. To provide customer authentication, use or create a plugin that implements [GrpcAuthenticationProvider](https://github.com/opensearch-project/data-prepper/blob/1.2.0/data-prepper-plugins/armeria-common/src/main/java/com/amazon/dataprepper/armeria/authentication/GrpcAuthenticationProvider.java). ## Metrics ### Counters - `requestTimeouts`: Measures the total number of requests that time out. - `requestsReceived`: Measures the total number of requests received by the `otel_trace` source. - `successRequests`: Measures the total number of requests successfully processed by the `otel_trace` source plugin. - `badRequests`: Measures the total number of requests with an invalid format processed by the `otel_trace` source plugin. - `requestsTooLarge`: Measures the total number of requests whose number of spans exceeds the buffer capacity. - `internalServerError`: Measures the total number of requests processed by the `otel_trace` source with a custom exception type. ### Timers - `requestProcessDuration`: Measures the latency of requests processed by the `otel_trace` source plugin in seconds. ### Distribution summaries - `payloadSize`: Measures the incoming request payload size distribution in bytes. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sources/otel-trace/",
    "relUrl": "/pipelines/configuration/sources/otel-trace/"
  },"39": {
    "doc": "parse_json",
    "title": "parse_json",
    "content": "# parse_json The `parse_json` processor parses JSON data for an event, including any nested fields. The processor extracts the JSON pointer data and adds the input event to the extracted fields. ## Configuration You can configure the `parse_json` processor with the following options. | Option | Required | Type | Description | :--- | :--- | :--- | :--- | `source` | No | String | The field in the `event` that will be parsed. Default value is `message`. | `destination` | No | String | The destination field of the parsed JSON. Defaults to the root of the `event`. Cannot be `\"\"`, `/`, or any whitespace-only `string` because these are not valid `event` fields. | `pointer` | No | String | A JSON pointer to the field to be parsed. There is no `pointer` by default, meaning the entire `source` is parsed. The `pointer` can access JSON array indexes as well. If the JSON pointer is invalid then the entire `source` data is parsed into the outgoing `event`. If the key that is pointed to already exists in the `event` and the `destination` is the root, then the pointer uses the entire path of the key. | ## Usage To get started, create the following `pipeline.yaml` file: ```yaml parse-json-pipeline: source: ...... processor: - parse_json: ``` ### Basic example To test the `parse_json` processor with the previous configuration, run the pipeline and paste the following line into your console, then enter `exit` on a new line: ``` {\"outer_key\": {\"inner_key\": \"inner_value\"}} ``` {% include copy.html %} The `parse_json` processor parses the message into the following format: ``` {\"message\": {\"outer_key\": {\"inner_key\": \"inner_value\"}}\", \"outer_key\":{\"inner_key\":\"inner_value\"}}} ``` ### Example with a JSON pointer You can use a JSON pointer to parse a selection of the JSON data by specifying the `pointer` option in the configuration. To get started, create the following `pipeline.yaml` file: ```yaml parse-json-pipeline: source: ...... processor: - parse_json: pointer: \"outer_key/inner_key\" ``` To test the `parse_json` processor with the pointer option, run the pipeline, paste the following line into your console, and then enter `exit` on a new line: ``` {\"outer_key\": {\"inner_key\": \"inner_value\"}} ``` {% include copy.html %} The processor parses the message into the following format: ``` {\"message\": {\"outer_key\": {\"inner_key\": \"inner_value\"}}\", \"inner_key\": \"inner_value\"} ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/parse-json/",
    "relUrl": "/pipelines/configuration/processors/parse-json/"
  },"40": {
    "doc": "Peer forwarder",
    "title": "Peer forwarder",
    "content": "# Peer forwarder Peer forwarder is an HTTP service that performs peer forwarding of an `event` between Data Prepper nodes for aggregation. This HTTP service uses a hash-ring approach to aggregate events and determine which Data Prepper node it should handle on a given trace before rerouting it to that node. Currently, peer forwarder is supported by the `aggregate`, `service_map_stateful`, and `otel_trace_raw` [processors]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/processors/). Peer forwarder groups events based on the identification keys provided by the supported processors. For `service_map_stateful` and `otel_trace_raw`, the identification key is `traceId` by default and cannot be configured. The `aggregate` processor is configured using the `identification_keys` configuration option. From here, you can specify which keys to use for peer forwarder. See [Aggregate Processor page](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/aggregate-processor#identification_keys) for more information about identification keys. Peer discovery allows Data Prepper to find other nodes that it will communicate with. Currently, peer discovery is provided by a static list, a DNS record lookup, or AWS Cloud Map. ## Discovery modes The following sections provide information about discovery modes. ### Static Static discovery mode allows a Data Prepper node to discover nodes using a list of IP addresses or domain names. See the following YAML file for an example of static discovery mode: ```yaml peer_forwarder:4 discovery_mode: static static_endpoints: [\"data-prepper1\", \"data-prepper2\"] ``` ### DNS lookup DNS discovery is preferred over static discovery when scaling out a Data Prepper cluster. DNS discovery configures a DNS provider to return a list of Data Prepper hosts when given a single domain name. This list consists of a [DNS A record](https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/), and a list of IP addresses of a given domain. See the following YAML file for an example of DNS lookup: ```yaml peer_forwarder: discovery_mode: dns domain_name: \"data-prepper-cluster.my-domain.net\" ``` ### AWS Cloud Map [AWS Cloud Map](https://docs.aws.amazon.com/cloud-map/latest/dg/what-is-cloud-map.html) provides API-based service discovery as well as DNS-based service discovery. Peer forwarder can use the API-based service discovery in AWS Cloud Map. To support this, you must have an existing namespace configured for API instance discovery. You can create a new one by following the instructions provided by the [AWS Cloud Map documentation](https://docs.aws.amazon.com/cloud-map/latest/dg/working-with-namespaces.html). Your Data Prepper configuration needs to include the following: * `aws_cloud_map_namespace_name`  Set to your AWS Cloud Map namespace name. * `aws_cloud_map_service_name`  Set to the service name within your specified namespace. * `aws_region`  Set to the AWS Region in which your namespace exists. * `discovery_mode`  Set to `aws_cloud_map`. Your Data Prepper configuration can optionally include the following: * `aws_cloud_map_query_parameters`  Key-value pairs are used to filter the results based on the custom attributes attached to an instance. Results include only those instances that match all of the specified key-value pairs. #### Example configuration See the following YAML file example of AWS Cloud Map configuration: ```yaml peer_forwarder: discovery_mode: aws_cloud_map aws_cloud_map_namespace_name: \"my-namespace\" aws_cloud_map_service_name: \"data-prepper-cluster\" aws_cloud_map_query_parameters: instance_type: \"r5.xlarge\" aws_region: \"us-east-1\" ``` ### IAM policy with necessary permissions Data Prepper must also be running with the necessary permissions. The following AWS Identity and Access Management (IAM) policy shows the necessary permissions: ```json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CloudMapPeerForwarder\", \"Effect\": \"Allow\", \"Action\": \"servicediscovery:DiscoverInstances\", \"Resource\": \"*\" } ] } ``` ## Configuration The following table provides optional configuration values. | Value | Type | Description | ---- | --- | ----------- | `port` | Integer | A value between 0 and 65535 that represents the port that the peer forwarder server is running on. Default value is `4994`. | `request_timeout` | Integer | Represents the request timeout duration in milliseconds for the peer forwarder HTTP server. Default value is `10000`. | `server_thread_count` | Integer | Represents the number of threads used by the peer forwarder server. Default value is `200`.| `client_thread_count` | Integer | Represents the number of threads used by the peer forwarder client. Default value is `200`.| `maxConnectionCount` | Integer | Represents the maximum number of open connections for the peer forwarder server. Default value is `500`. | `discovery_mode` | String | Represents the peer discovery mode to be used. Allowable values are `local_node`, `static`, `dns`, and `aws_cloud_map`. Defaults to `local_node`, which processes events locally. | `static_endpoints` | List | Contains the endpoints of all Data Prepper instances. Required if `discovery_mode` is set to `static`. | `domain_name` | String | Represents the single domain name to query DNS against. Typically used by creating multiple [DNS A records](https://www.cloudflare.com/learning/dns/dns-records/dns-a-record/) for the same domain. Required if `discovery_mode` is set to `dns`. | `aws_cloud_map_namespace_name` | String | Represents the AWS Cloud Map namespace when using AWS Cloud Map service discovery. Required if `discovery_mode` is set to `aws_cloud_map`. | `aws_cloud_map_service_name` | String | Represents the AWS Cloud Map service when using AWS Cloud Map service discovery. Required if `discovery_mode` is set to `aws_cloud_map`. | `aws_cloud_map_query_parameters` | Map | Key-value pairs used to filter the results based on the custom attributes attached to an instance. Only instances that match all the specified key-value pairs are returned. | `buffer_size` | Integer | Represents the maximum number of unchecked records the buffer accepts (the number of unchecked records equals the number of records written into the buffer plus the number of records that are still processing and not yet checked by the Checkpointing API). Default is `512`. | `batch_size` | Integer | Represents the maximum number of records that the buffer returns on read. Default is `48`. | `aws_region` | String | Represents the AWS Region that uses `ACM`, `Amazon S3`, or `AWS Cloud Map` and is required when any of the following conditions are met: - The `use_acm_certificate_for_ssl` setting is set to `true`. - Either `ssl_certificate_file` or `ssl_key_file` specifies an Amazon Simple Storage Service (Amazon S3) URI (for example, s3://mybucket/path/to/public.cert). - The `discovery_mode` is set to `aws_cloud_map`. | `drain_timeout` | Duration | Represents the amount of time that peer forwarder will wait to complete data processing before shutdown. | ## SSL configuration The following table provides optional SSL configuration values that allow you to set up a trust manager for the peer forwarder client in order to connect to other Data Prepper instances. | Value | Type | Description | ----- | ---- | ----------- | `ssl` | Boolean | Enables TLS/SSL. Default value is `true`. | `ssl_certificate_file`| String | Represents the SSL certificate chain file path or Amazon S3 path. The following is an example of an Amazon S3 path: `s3:///`. Defaults to the default certificate file,`config/default_certificate.pem`. See [Default Certificates](https://github.com/opensearch-project/data-prepper/tree/main/examples/certificates) for more information about how the certificate is generated. | `ssl_key_file`| String | Represents the SSL key file path or Amazon S3 path. Amazon S3 path example: `s3:///`. Defaults to `config/default_private_key.pem` which is the default private key file. See [Default Certificates](https://github.com/opensearch-project/data-prepper/tree/main/examples/certificates) for more information about how the private key file is generated. | `ssl_insecure_disable_verification` | Boolean | Disables the verification of the server's TLS certificate chain. Default value is `false`. | `ssl_fingerprint_verification_only` | Boolean | Disables the verification of the server's TLS certificate chain and instead verifies only the certificate fingerprint. Default value is `false`. | `use_acm_certificate_for_ssl` | Boolean | Enables TLS/SSL using the certificate and private key from AWS Certificate Manager (ACM). Default value is `false`. | `acm_certificate_arn`| String | Represents the ACM certificate Amazon Resource Name (ARN). The ACM certificate takes precedence over Amazon S3 or the local file system certificate. Required if `use_acm_certificate_for_ssl` is set to `true`. | `acm_private_key_password` | String | Represents the ACM private key password that will be used to decrypt the private key. If it's not provided, a random password will be generated. | `acm_certificate_timeout_millis` | Integer | Represents the timeout in milliseconds required for ACM to get certificates. Default value is `120000`. | `aws_region` | String | Represents the AWS Region that uses ACM, Amazon S3, or AWS Cloud Map. Required if `use_acm_certificate_for_ssl` is set to `true` or `ssl_certificate_file`. Also required when the `ssl_key_file` is set to use the Amazon S3 path or if `discovery_mode` is set to `aws_cloud_map`. | #### Example configuration The following YAML file provides an example configuration: ```yaml peer_forwarder: ssl: true ssl_certificate_file: \"\" ssl_key_file: \"\" ``` ## Authentication `Authentication` is optional and is a `Map` that enables mutual TLS (mTLS). It can either be `mutual_tls` or `unauthenticated`. The default value is `unauthenticated`. The following YAML file provides an example of authentication: ```yaml peer_forwarder: authentication: mutual_tls: ``` ## Metrics Core peer forwarder introduces the following custom metrics. All the metrics are prefixed by `core.peerForwarder`. ### Timer Peer forwarder's timer capability provides the following information: - `requestForwardingLatency`: Measures latency of requests forwarded by the peer forwarder client. - `requestProcessingLatency`: Measures latency of requests processed by the peer forwarder server. ### Counter The following table provides counter metric options. | Value | Description | ----- | ----------- | `requests`| Measures the total number of forwarded requests. | `requestsFailed`| Measures the total number of failed requests. Applies to requests with an HTTP response code other than `200`. | `requestsSuccessful`| Measures the total number of successful requests. Applies to requests with HTTP response code `200`. | `requestsTooLarge`| Measures the total number of requests that are too large to be written to the peer forwarder buffer. Applies to requests with HTTP response code `413`. | `requestTimeouts`| Measures the total number of requests that time out while writing content to the peer forwarder buffer. Applies to requests with HTTP response code `408`. | `requestsUnprocessable`| Measures the total number of requests that fail due to an unprocessable entity. Applies to requests with HTTP response code `422`. | `badRequests`| Measures the total number of requests with a bad request format. Applies to requests with HTTP response code `400`. | `recordsSuccessfullyForwarded`| Measures the total number of successfully forwarded records. | `recordsFailedForwarding`| Measures the total number of records that fail to be forwarded. | `recordsToBeForwarded` | Measures the total number of records to be forwarded. | `recordsToBeProcessedLocally` | Measures the total number of records to be processed locally. | `recordsActuallyProcessedLocally`| Measures the total number of records actually processed locally. This value is the sum of `recordsToBeProcessedLocally` and `recordsFailedForwarding`. | `recordsReceivedFromPeers`| Measures the total number of records received from remote peers. | ### Gauge `peerEndpoints` Measures the number of dynamically discovered peer Data Prepper endpoints. For `static` mode, the size is fixed. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/managing-data-prepper/peer-forwarder/",
    "relUrl": "/managing-data-prepper/peer-forwarder/"
  },"41": {
    "doc": "Pipeline sink",
    "title": "Pipeline sink",
    "content": "# Pipeline sink ## Overview You can use the `pipeline` sink to write to another pipeline. Option | Required | Type | Description :--- | :--- | :--- | :--- name | Yes | String | Name of the pipeline to write to. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sinks/pipeline/",
    "relUrl": "/pipelines/configuration/sinks/pipeline/"
  },"42": {
    "doc": "Pipeline options",
    "title": "Pipeline options",
    "content": "# Pipeline options This page provides information about pipeline configuration options in Data Prepper. ## General pipeline options Option | Required | Type | Description :--- | :--- | :--- | :--- workers | No | Integer | Essentially the number of application threads. As a starting point for your use case, try setting this value to the number of CPU cores on the machine. Default is 1. delay | No | Integer | Amount of time in milliseconds workers wait between buffer read attempts. Default is 3,000. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/pipelines-configuration-options/",
    "relUrl": "/pipelines/pipelines-configuration-options/"
  },"43": {
    "doc": "Pipelines",
    "title": "Pipelines",
    "content": "# Pipelines The following image illustrates how a pipeline works. {: .img-fluid} To use Data Prepper, you define pipelines in a configuration YAML file. Each pipeline is a combination of a source, a buffer, zero or more processors, and one or more sinks. For example: ```yml simple-sample-pipeline: workers: 2 # the number of workers delay: 5000 # in milliseconds, how long workers wait between read attempts source: random: buffer: bounded_blocking: buffer_size: 1024 # max number of records the buffer accepts batch_size: 256 # max number of records the buffer drains after each read processor: - string_converter: upper_case: true sink: - stdout: ``` - Sources define where your data comes from. In this case, the source is a random UUID generator (`random`). - Buffers store data as it passes through the pipeline. By default, Data Prepper uses its one and only buffer, the `bounded_blocking` buffer, so you can omit this section unless you developed a custom buffer or need to tune the buffer settings. - Processors perform some action on your data: filter, transform, enrich, etc. You can have multiple processors, which run sequentially from top to bottom, not in parallel. The `string_converter` processor transform the strings by making them uppercase. - Sinks define where your data goes. In this case, the sink is stdout. Starting from Data Prepper 2.0, you can define pipelines across multiple configuration YAML files, where each file contains the configuration for one or more pipelines. This gives you more freedom to organize and chain complex pipeline configurations. For Data Prepper to load your pipeline configuration properly, place your configuration YAML files in the `pipelines` folder under your application's home directory (e.g. `/usr/share/data-prepper`). {: .note } ## End-to-end acknowledgments Data Prepper ensures the durability and reliability of data written from sources and delivered to sinks through end-to-end (E2E) acknowledgments. An E2E acknowledgment begins at the source, which monitors a batch of events set inside pipelines and waits for a positive acknowledgment when those events are successfully pushed to sinks. When a pipeline contains multiple sinks, including sinks set as additional Data Prepper pipelines, the E2E acknowledgment sends when events are received by the final sink in a pipeline chain. Alternatively, the source sends a negative acknowledgment when an event cannot be delivered to a sink for any reason. When any component of a pipeline fails and is unable to send an event, the source receives no acknowledgment. In the case of a failure, the pipeline's source times out. This gives you the ability to take any necessary actions to address the source failure, including rerunning the pipeline or logging the failure. As of Data Prepper 2.2, only the `s3` source and `opensearch` sink support E2E acknowledgments. ## Conditional routing Pipelines also support **conditional routing** which allows you to route Events to different sinks based on specific conditions. To add conditional routing to a pipeline, specify a list of named routes under the `route` component and add specific routes to sinks under the `routes` property. Any sink with the `routes` property will only accept Events that match at least one of the routing conditions. In the following example, `application-logs` is a named route with a condition set to `/log_type == \"application\"`. The route uses [Data Prepper expressions](https://github.com/opensearch-project/data-prepper/tree/main/examples) to define the conditions. Data Prepper only routes events that satisfy the condition to the first OpenSearch sink. By default, Data Prepper routes all Events to a sink which does not define a route. In the example, all Events route into the third OpenSearch sink. ```yml conditional-routing-sample-pipeline: source: http: processor: route: - application-logs: '/log_type == \"application\"' - http-logs: '/log_type == \"apache\"' sink: - opensearch: hosts: [ \"https://opensearch:9200\" ] index: application_logs routes: [application-logs] - opensearch: hosts: [ \"https://opensearch:9200\" ] index: http_logs routes: [http-logs] - opensearch: hosts: [ \"https://opensearch:9200\" ] index: all_logs ``` ## Examples This section provides some pipeline examples that you can use to start creating your own pipelines. For more pipeline configurations, select from the following options for each component: - [Buffers]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/buffers/buffers/) - [Processors]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/processors/) - [Sinks]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sinks/sinks/) - [Sources]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sources/sources/) The Data Prepper repository has several [sample applications](https://github.com/opensearch-project/data-prepper/tree/main/examples) to help you get started. ### Log ingestion pipeline The following example `pipeline.yaml` file with SSL and basic authentication enabled for the `http-source` demonstrates how to use the HTTP Source and Grok Prepper plugins to process unstructured log data: ```yaml log-pipeline: source: http: ssl_certificate_file: \"/full/path/to/certfile.crt\" ssl_key_file: \"/full/path/to/keyfile.key\" authentication: http_basic: username: \"myuser\" password: \"mys3cret\" processor: - grok: match: # This will match logs with a \"log\" key against the COMMONAPACHELOG pattern (ex: { \"log\": \"actual apache log...\" } ) # You should change this to match what your logs look like. See the grok documenation to get started. log: [ \"%{COMMONAPACHELOG}\" ] sink: - opensearch: hosts: [ \"https://localhost:9200\" ] # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 # Since we are grok matching for apache logs, it makes sense to send them to an OpenSearch index named apache_logs. # You should change this to correspond with how your OpenSearch indices are set up. index: apache_logs ``` This example uses weak security. We strongly recommend securing all plugins which open external ports in production environments. {: .note} ### Trace analytics pipeline The following example demonstrates how to build a pipeline that supports the [Trace Analytics OpenSearch Dashboards plugin]({{site.url}}{{site.baseurl}}/observability-plugin/trace/ta-dashboards/). This pipeline takes data from the OpenTelemetry Collector and uses two other pipelines as sinks. These two separate pipelines index trace and the service map documents for the dashboard plugin. Starting from Data Prepper 2.0, Data Prepper no longer supports `otel_trace_raw_prepper` processor due to the Data Prepper internal data model evolution. Instead, users should use `otel_trace_raw`. ```yml entry-pipeline: delay: \"100\" source: otel_trace_source: ssl: false buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 sink: - pipeline: name: \"raw-pipeline\" - pipeline: name: \"service-map-pipeline\" raw-pipeline: source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - otel_trace_raw: sink: - opensearch: hosts: [\"https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-raw service-map-pipeline: delay: \"100\" source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - service_map_stateful: sink: - opensearch: hosts: [\"https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-service-map ``` To maintain similar ingestion throughput and latency, scale the `buffer_size` and `batch_size` by the estimated maximum batch size in the client request payload. {: .tip} ### Metrics pipeline Data Prepper supports metrics ingestion using OTel. It currently supports the following metric types: * Gauge * Sum * Summary * Histogram Other types are not supported. Data Prepper drops all other types, including Exponential Histogram and Summary. Additionally, Data Prepper does not support Scope instrumentation. To set up a metrics pipeline: ```yml metrics-pipeline: source: otel_metrics_source: processor: - otel_metrics_raw_processor: sink: - opensearch: hosts: [\"https://localhost:9200\"] username: admin password: admin ``` ### S3 log ingestion pipeline The following example demonstrates how to use the S3Source and Grok Processor plugins to process unstructured log data from [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3). This example uses application load balancer logs. As the application load balancer writes logs to S3, S3 creates notifications in Amazon SQS. Data Prepper monitors those notifications and reads the S3 objects to get the log data and process it. ```yml log-pipeline: source: s3: notification_type: \"sqs\" compression: \"gzip\" codec: newline: sqs: queue_url: \"https://sqs.us-east-1.amazonaws.com/12345678910/ApplicationLoadBalancer\" aws: region: \"us-east-1\" sts_role_arn: \"arn:aws:iam::12345678910:role/Data-Prepper\" processor: - grok: match: message: [\"%{DATA:type} %{TIMESTAMP_ISO8601:time} %{DATA:elb} %{DATA:client} %{DATA:target} %{BASE10NUM:request_processing_time} %{DATA:target_processing_time} %{BASE10NUM:response_processing_time} %{BASE10NUM:elb_status_code} %{DATA:target_status_code} %{BASE10NUM:received_bytes} %{BASE10NUM:sent_bytes} \\\"%{DATA:request}\\\" \\\"%{DATA:user_agent}\\\" %{DATA:ssl_cipher} %{DATA:ssl_protocol} %{DATA:target_group_arn} \\\"%{DATA:trace_id}\\\" \\\"%{DATA:domain_name}\\\" \\\"%{DATA:chosen_cert_arn}\\\" %{DATA:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \\\"%{DATA:actions_executed}\\\" \\\"%{DATA:redirect_url}\\\" \\\"%{DATA:error_reason}\\\" \\\"%{DATA:target_list}\\\" \\\"%{DATA:target_status_code_list}\\\" \\\"%{DATA:classification}\\\" \\\"%{DATA:classification_reason}\"] - grok: match: request: [\"(%{NOTSPACE:http_method})? (%{NOTSPACE:http_uri})? (%{NOTSPACE:http_version})?\"] - grok: match: http_uri: [\"(%{WORD:protocol})?(://)?(%{IPORHOST:domain})?(:)?(%{INT:http_port})?(%{GREEDYDATA:request_uri})?\"] - date: from_time_received: true destination: \"@timestamp\" sink: - opensearch: hosts: [ \"https://localhost:9200\" ] username: \"admin\" password: \"admin\" index: alb_logs ``` ## Migrating from Logstash Data Prepper supports Logstash configuration files for a limited set of plugins. Simply use the logstash config to run Data Prepper. ```bash docker run --name data-prepper \\ -v /full/path/to/logstash.conf:/usr/share/data-prepper/pipelines/pipelines.conf \\ opensearchproject/opensearch-data-prepper:latest ``` This feature is limited by feature parity of Data Prepper. As of Data Prepper 1.2 release, the following plugins from the Logstash configuration are supported: - HTTP Input plugin - Grok Filter plugin - Elasticsearch Output plugin - Amazon Elasticsearch Output plugin ## Configure the Data Prepper server Data Prepper itself provides administrative HTTP endpoints such as `/list` to list pipelines and `/metrics/prometheus` to provide Prometheus-compatible metrics data. The port that has these endpoints has a TLS configuration and is specified by a separate YAML file. By default, these endpoints are secured by Data Prepper docker images. We strongly recommend providing your own configuration file for securing production environments. Here is an example `data-prepper-config.yaml`: ```yml ssl: true keyStoreFilePath: \"/usr/share/data-prepper/keystore.jks\" keyStorePassword: \"password\" privateKeyPassword: \"other_password\" serverPort: 1234 ``` To configure the Data Prepper server, run Data Prepper with the additional yaml file. ```bash docker run --name data-prepper \\ -v /full/path/to/my-pipelines.yaml:/usr/share/data-prepper/pipelines/my-pipelines.yaml \\ -v /full/path/to/data-prepper-config.yaml:/usr/share/data-prepper/data-prepper-config.yaml \\ opensearchproject/data-prepper:latest ``` ## Configure peer forwarder Data Prepper provides an HTTP service to forward Events between Data Prepper nodes for aggregation. This is required for operating Data Prepper in a clustered deployment. Currently, peer forwarding is supported in `aggregate`, `service_map_stateful`, and `otel_trace_raw` processors. Peer forwarder groups events based on the identification keys provided by the processors. For `service_map_stateful` and `otel_trace_raw` it's `traceId` by default and can not be configured. For `aggregate` processor, it is configurable using `identification_keys` option. Peer forwarder supports peer discovery through one of three options: a static list, a DNS record lookup , or AWS Cloud Map. Peer discovery can be configured using `discovery_mode` option. Peer forwarder also supports SSL for verification and encryption, and mTLS for mutual authentication in a peer forwarding service. To configure peer forwarder, add configuration options to `data-prepper-config.yaml` mentioned in the [Configure the Data Prepper server](#configure-the-data-prepper-server) section: ```yml peer_forwarder: discovery_mode: dns domain_name: \"data-prepper-cluster.my-domain.net\" ssl: true ssl_certificate_file: \"\" ssl_key_file: \"\" authentication: mutual_tls: ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/pipelines/",
    "relUrl": "/pipelines/pipelines/"
  },"44": {
    "doc": "Processors",
    "title": "Processors",
    "content": "# Processors Processors perform an action on your data, such as filtering, transforming, or enriching. Prior to Data Prepper 1.3, processors were named preppers. Starting in Data Prepper 1.3, the term *prepper* is deprecated in favor of the term *processor*. Data Prepper will continue to support the term *prepper* until 2.0, where it will be removed. {: .note } ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/processors/",
    "relUrl": "/pipelines/configuration/processors/processors/"
  },"45": {
    "doc": "rename_keys",
    "title": "rename_keys",
    "content": "# rename_keys The `rename_keys` processor renames keys in an event. ## Configuration You can configure the `rename_keys` processor with the following options. | Option | Required | Description | :--- | :--- | :--- | `entries` | Yes | A list of event entries to rename. | `from_key` | Yes | The key of the entry to be renamed. | `to_key` | Yes | The new key of the entry. | `overwrite_if_to_key_exists` | No | When set to `true`, the existing value is overwritten if `key` already exists in the event. The default value is `false`. | ## Usage To get started, create the following `pipeline.yaml` file: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - rename_keys: entries: - from_key: \"message\" to_key: \"newMessage\" overwrite_if_to_key_exists: true sink: - stdout: ``` {% include copy.html %} Next, create a log file named `logs_json.log` and replace the `path` in the file source of your `pipeline.yaml` file with that filepath. For more information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). For example, before you run the `rename_keys` processor, if the `logs_json.log` file contains the following event record: ```json {\"message\": \"hello\"} ``` When you run the `rename_keys` processor, it parses the message into the following \"newMessage\" output: ```json {\"newMessage\": \"hello\"} ``` > If `newMessage` already exists, its existing value is overwritten with `value`. ## Special considerations Renaming operations occur in the order that the key-value pair entries are listed in the `pipeline.yaml` file. This means that chaining (where key-value pairs are renamed in sequence) is implicit in the `rename_keys` processor. See the following example `pipline.yaml` file: ```yaml pipeline: source: file: path: \"/full/path/to/logs_json.log\" record_type: \"event\" format: \"json\" processor: - rename_keys: entries: - from_key: \"message\" to_key: \"message2\" - from_key: \"message2\" to_key: \"message3\" sink: - stdout: ``` Add the following contents to the `logs_json.log` file: ```json {\"message\": \"hello\"} ``` {% include copy.html %} After the `rename_keys` processor runs, the following output appears: ```json {\"message3\": \"hello\"} ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/rename-keys/",
    "relUrl": "/pipelines/configuration/processors/rename-keys/"
  },"46": {
    "doc": "routes",
    "title": "routes",
    "content": "# Routes Routes define conditions that can be used in sinks for conditional routing. Routes are specified at the same level as processors and sinks under the name `route` and consist of a list of key-value pairs, where the key is the name of a route and the value is a Data Prepper expression representing the routing condition. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/routes/",
    "relUrl": "/pipelines/configuration/processors/routes/"
  },"47": {
    "doc": "S3 logs",
    "title": "S3 logs",
    "content": "# S3 logs Data Prepper allows you to load logs from [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3), including traditional logs, JSON documents, and CSV logs. ## Architecture Data Prepper can read objects from S3 buckets using an [Amazon Simple Queue Service (SQS)](https://aws.amazon.com/sqs/) (Amazon SQS) queue and [Amazon S3 Event Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html). Data Prepper polls the Amazon SQS queue for S3 event notifications. When Data Prepper receives a notification that an S3 object was created, Data Prepper reads and parses that S3 object. The following diagram shows the overall architecture of the components involved. {: .img-fluid} The flow of data is as follows. 1. A system produces logs into the S3 bucket. 2. S3 creates an S3 event notification in the SQS queue. 3. Data Prepper polls Amazon SQS for messages and then receives a message. 4. Data Prepper downloads the content from the S3 object. 5. Data Prepper sends a document to OpenSearch for the content in the S3 object. ## Pipeline overview Data Prepper supports reading data from S3 using the [`s3` source]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sources/s3/). The following diagram shows a conceptual outline of a Data Prepper pipeline reading from S3. {: .img-fluid} ## Prerequisites Before Data Prepper can read log data from S3, you need the following prerequisites: - An S3 bucket. - A log producer that writes logs to S3. The exact log producer will vary depending on your specific use case, but could include writing logs to S3 or a service such as Amazon CloudWatch. ## Getting started Use the following steps to begin loading logs from S3 with Data Prepper. 1. Create an [SQS standard queue](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/step-create-queue.html) for your S3 event notifications. 2. Configure [bucket notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html) for SQS. Use the `s3:ObjectCreated:*` event type. 3. Grant [AWS IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html) permissions to Data Prepper for accessing SQS and S3. 4. (Recommended) Create an [SQS dead-letter queue](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html) (DLQ). 5. (Recommended) Configure an SQS re-drive policy to move failed messages into the DLQ. ### Setting permissions for Data Prepper To view S3 logs, Data Prepper needs access to Amazon SQS and S3. Use the following example to set up permissions: ```json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"s3-access\", \"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::/*\" }, { \"Sid\": \"sqs-access\", \"Effect\": \"Allow\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:ReceiveMessage\" ], \"Resource\": \"arn:aws:sqs:::\" }, { \"Sid\": \"kms-access\", \"Effect\": \"Allow\", \"Action\": \"kms:Decrypt\", \"Resource\": \"arn:aws:kms:::key/\" } ] } ``` If your S3 objects or SQS queues do not use KMS, you can remove the `kms:Decrypt` permission. ### SQS dead-letter queue The are two options for how to handle errors resulting from processing S3 objects. - Use an SQS dead-letter queue (DLQ) to track the failure. This is the recommended approach. - Delete the message from SQS. You must manually find the S3 object and correct the error. The following diagram shows the system architecture when using SQS with DLQ. {: .img-fluid} To use an SQS dead-letter queue, perform the following steps: 1. Create a new SQS standard queue to act as your DLQ. 2. Configure your SQS's redrive policy [to use your DLQ](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-dead-letter-queue.html). Consider using a low value such as 2 or 3 for the \"Maximum Receives\" setting. 3. Configure the Data Prepper `s3` source to use `retain_messages` for `on_error`. This is the default behavior. ## Pipeline design Create a pipeline to read logs from S3, starting with an [`s3`]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sources/s3/) source plugin. Use the following example for guidance. ```yaml s3-log-pipeline: source: s3: notification_type: sqs compression: gzip codec: newline: sqs: # Change this value to your SQS Queue URL queue_url: \"arn:aws:sqs:::\" visibility_timeout: \"2m\" ``` Configure the following options according to your use case: * `queue_url`: This the SQS queue URL and is always unique to your pipeline. * `codec`: The codec determines how to parse the incoming data. * `visibility_timeout`: Configure this value to be large enough for Data Prepper to process 10 S3 objects. However, if you make this value too large, messages that fail to process will take at least as long as the specified value before Data Prepper retries. The default values for each option work for the majority of use cases. For all available options for the S3 source, see [`s3`]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sources/s3/). ```yaml s3-log-pipeline: source: s3: notification_type: sqs compression: gzip codec: newline: sqs: # Change this value to your SQS Queue URL queue_url: \"arn:aws:sqs:::\" visibility_timeout: \"2m\" aws: # Specify the correct region region: \"\" # This shows using an STS role, but you can also use your system's default permissions. sts_role_arn: \"arn:aws:iam:::role/\" processor: # You can configure a grok pattern to enrich your documents in OpenSearch. #- grok: # match: # message: [ \"%{COMMONAPACHELOG}\" ] sink: - opensearch: hosts: [ \"https://localhost:9200\" ] # Change to your credentials username: \"admin\" password: \"admin\" index: s3_logs ``` ## Multiple Data Prepper pipelines We recommend that you have one SQS queue per Data Prepper pipeline. In addition, you can have multiple nodes in the same cluster reading from the same SQS queue, which doesn't require additional configuration with Data Prepper. If you have multiple pipelines, you must create multiple SQS queues for each pipeline, even if both pipelines use the same S3 bucket. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/common-use-cases/s3-logs/",
    "relUrl": "/common-use-cases/s3-logs/"
  },"48": {
    "doc": "s3 source",
    "title": "s3 source",
    "content": "# s3 source `s3` is a source plugin that reads events from [Amazon Simple Storage Service (Amazon S3)](https://aws.amazon.com/s3/) objects. It requires an [Amazon Simple Queue Service (Amazon SQS)](https://aws.amazon.com/sqs/) queue that receives [S3 Event Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html). After Amazon SQS is configured, the `s3` source receives messages from Amazon SQS. When the SQS message indicates that an S3 object was created, the `s3` source loads the S3 objects and then parses them using the configured [codec](#codec). You can also configure the `s3` source to use [Amazon S3 Select](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html) instead of Data Prepper to parse S3 objects. ## IAM permissions In order to use the `s3` source, configure your AWS Identity and Access Management (IAM) permissions to grant Data Prepper access to Amazon S3. You can use a configuration similar to the following JSON configuration: ```json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"s3-access\", \"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::/*\" }, { \"Sid\": \"sqs-access\", \"Effect\": \"Allow\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:ReceiveMessage\" ], \"Resource\": \"arn:aws:sqs:::\" }, { \"Sid\": \"kms-access\", \"Effect\": \"Allow\", \"Action\": \"kms:Decrypt\", \"Resource\": \"arn:aws:kms:::key/\" } ] } ``` If your S3 objects or Amazon SQS queues do not use [AWS Key Management Service (AWS KMS)](https://aws.amazon.com/kms/), remove the `kms:Decrypt` permission. ## Configuration You can use the following options to configure the `s3` source. Option | Required | Type | Description :--- | :--- | :--- | :--- notification_type | Yes | String | Must be `sqs`. compression | No | String | The compression algorithm to apply: `none`, `gzip`, or `automatic`. Default value is `none`. codec | Yes | Codec | The [codec](#codec) to apply. sqs | Yes | sqs | The SQS configuration. See [sqs](#sqs) for details. aws | Yes | aws | The AWS configuration. See [aws](#aws) for details. on_error | No | String | Determines how to handle errors in Amazon SQS. Can be either `retain_messages` or `delete_messages`. If `retain_messages`, then Data Prepper will leave the message in the Amazon SQS queue and try again. This is recommended for dead-letter queues. If `delete_messages`, then Data Prepper will delete failed messages. Default value is `retain_messages`. buffer_timeout | No | Duration | The amount of time allowed for for writing events to the Data Prepper buffer before timeout occurs. Any events that the Amazon S3 source cannot write to the buffer in this time will be discarded. Default value is 10 seconds. records_to_accumulate | No | Integer | The number of messages that accumulate before writing to the buffer. Default value is 100. metadata_root_key | No | String | Base key for adding S3 metadata to each Event. The metadata includes the key and bucket for each S3 object. Defaults to `s3/`. disable_bucket_ownership_validation | No | Boolean | If `true`, the S3Source will not attempt to validate that the bucket is owned by the expected account. The expected account is the same account that owns the Amazon SQS queue. Defaults to `false`. acknowledgments | No | Boolean | If `true`, enables `s3` sources to receive [end-to-end acknowledgments]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/pipelines/#end-to-end-acknowledgments) when events are received by OpenSearch sinks. ## sqs The following parameters allow you to configure usage for Amazon SQS in the `s3` source plugin. Option | Required | Type | Description :--- | :--- | :--- | :--- queue_url | Yes | String | The URL of the Amazon SQS queue from which messages are received. maximum_messages | No | Integer | The maximum number of messages to receive from the Amazon SQS queue in any single request. Default value is `10`. visibility_timeout | No | Duration | The visibility timeout to apply to messages read from the Amazon SQS queue. This should be set to the amount of time that Data Prepper may take to read all the S3 objects in a batch. Default value is `30s`. wait_time | No | Duration | The amount of time to wait for long polling on the Amazon SQS API. Default value is `20s`. poll_delay | No | Duration | A delay to place between reading/processing a batch of Amazon SQS messages and making a subsequent request. Default value is `0s`. ## aws Option | Required | Type | Description :--- | :--- | :--- | :--- region | No | String | The AWS Region to use for credentials. Defaults to [standard SDK behavior to determine the Region](https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/region-selection.html). sts_role_arn | No | String | The AWS Security Token Service (AWS STS) role to assume for requests to Amazon SQS and Amazon S3. Defaults to null, which will use the [standard SDK behavior for credentials](https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/credentials.html). aws_sts_header_overrides | No | Map | A map of header overrides that the IAM role assumes for the sink plugin. ## codec The `codec` determines how the `s3` source parses each S3 object. ### newline codec The `newline` codec parses each single line as a single log event. This is ideal for most application logs because each event parses per single line. It can also be suitable for S3 objects that have individual JSON objects on each line, which matches well when used with the [parse_json]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/parse-json/) processor to parse each line. Use the following options to configure the `newline` codec. Option | Required | Type | Description :--- | :--- |:--------| :--- skip_lines | No | Integer | The number of lines to skip before creating events. You can use this configuration to skip common header rows. Default is `0`. header_destination | No | String | A key value to assign to the header line of the S3 object. If this option is specified, then each event will contain a header_destination field. ### json codec The `json` codec parses each S3 object as a single JSON object from a JSON array and then creates a Data Prepper log event for each object in the array. ### csv codec The `csv` codec parses objects in comma-separated value (CSV) format, with each row producing a Data Prepper log event. Use the following options to configure the `csv` codec. Option | Required | Type | Description :--- |:---------|:------------| :--- delimiter | Yes | Integer | The delimiter separating columns. Default is `,`. quote_character | Yes | String | The character used as a text qualifier for CSV data. Default is `\"`. header | No | String list | The header containing the column names used to parse CSV data. detect_header | No | Boolean | Whether the first line of the S3 object should be interpreted as a header. Default is `true`. ## Using `s3_select` with the `s3` source When configuring `s3_select` to parse S3 objects, use the following options. Option | Required | Type | Description :--- |:-----------------------|:------------| :--- expression | Yes, when using `s3_select` | String | The expression used to query the object. Maps directly to the [expression](https://docs.aws.amazon.com/AmazonS3/latest/API/API_SelectObjectContent.html#AmazonS3-SelectObjectContent-request-Expression) property. expression_type | No | String | The type of the provided expression. Default value is `SQL`. Maps directly to the [ExpressionType](https://docs.aws.amazon.com/AmazonS3/latest/API/API_SelectObjectContent.html#AmazonS3-SelectObjectContent-request-ExpressionType). input_serialization | Yes, when using `s3_select` | String | Provides the S3 Select file format. Amazon S3 uses this format to parse object data into records and returns only records that match the specified SQL expression. May be `csv`, `json`, or `parquet`. compression_type | No | String | Specifies an object's compression format. Maps directly to the [CompressionType](https://docs.aws.amazon.com/AmazonS3/latest/API/API_InputSerialization.html#AmazonS3-Type-InputSerialization-CompressionType). csv | No | [csv](#s3_select_csv) | Provides the CSV configuration for processing CSV data. json | No | [json](#s3_select_json) | Provides the JSON configuration for processing JSON data. ### csv Use the following options in conjunction with the `csv` configuration for `s3_select` to determine how your parsed CSV file should be formatted. These options map directly to options available in the S3 Select [CSVInput](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CSVInput.html) data type. Option | Required | Type | Description :--- |:---------|:------------| :--- file_header_info | No | String | Describes the first line of input. Maps directly to the [FileHeaderInfo](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CSVInput.html#AmazonS3-Type-CSVInput-FileHeaderInfo) property. quote_escape | No | String | A single character used for escaping the quotation mark character inside an already escaped value. Maps directly to the [QuoteEscapeCharacter](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CSVInput.html#AmazonS3-Type-CSVInput-QuoteEscapeCharacter) property. comments | No | String | A single character used to indicate that a row should be ignored when the character is present at the start of that row. Maps directly to the [Comments](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CSVInput.html#AmazonS3-Type-CSVInput-Comments) property. #### json Use the following option in conjunction with `json` for `s3_select` to determine how S3 Select processes the JSON file. Option | Required | Type | Description :--- |:---------|:------------| :--- type | No | String | The type of JSON array. May be either `DOCUMENT` or `LINES`. Maps directly to the [Type](https://docs.aws.amazon.com/AmazonS3/latest/API/API_JSONInput.html#AmazonS3-Type-JSONInput-Type) property. ## Metrics The `s3` source includes the following metrics. ### Counters * `s3ObjectsFailed`: The number of S3 objects that the `s3` source failed to read. * `s3ObjectsNotFound`: The number of S3 objects that the `s3` source failed to read due to an S3 \"Not Found\" error. These are also counted toward `s3ObjectsFailed`. * `s3ObjectsAccessDenied`: The number of S3 objects that the `s3` source failed to read due to an \"Access Denied\" or \"Forbidden\" error. These are also counted toward `s3ObjectsFailed`. * `s3ObjectsSucceeded`: The number of S3 objects that the `s3` source successfully read. * `sqsMessagesReceived`: The number of Amazon SQS messages received from the queue by the `s3` source. * `sqsMessagesDeleted`: The number of Amazon SQS messages deleted from the queue by the `s3` source. * `sqsMessagesFailed`: The number of Amazon SQS messages that the `s3` source failed to parse. ### Timers * `s3ObjectReadTimeElapsed`: Measures the amount of time the `s3` source takes to perform a request to GET an S3 object, parse it, and write events to the buffer. * `sqsMessageDelay`: Measures the time elapsed from when S3 creates an object to when it is fully parsed. ### Distribution summaries * `s3ObjectSizeBytes`: Measures the size of S3 objects as reported by the S3 `Content-Length`. For compressed objects, this is the compressed size. * `s3ObjectProcessedBytes`: Measures the bytes processed by the `s3` source for a given object. For compressed objects, this is the uncompressed size. * `s3ObjectsEvents`: Measures the number of events (sometimes called records) produced by an S3 object. ## Example: Uncompressed logs The following pipeline.yaml file shows the minimum configuration for reading uncompressed newline-delimited logs: ``` source: s3: notification_type: sqs codec: newline: compression: none sqs: queue_url: \"https://sqs.us-east-1.amazonaws.com/123456789012/MyQueue\" aws: region: \"us-east-1\" sts_role_arn: \"arn:aws:iam::123456789012:role/Data-Prepper\" ``` ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sources/s3/",
    "relUrl": "/pipelines/configuration/sources/s3/"
  },"49": {
    "doc": "service_map",
    "title": "service_map",
    "content": "# service_map The `service_map` processor uses OpenTelemetry data to create a distributed service map for visualization in OpenSearch Dashboards. ## Configuration The following table describes the option you can use to configure the `service_map` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- window_duration | No | Integer | Represents the fixed time window, in seconds, during which service map relationships are evaluated. Default value is 180. ## Metrics The following table describes common [Abstract processor](https://github.com/opensearch-project/data-prepper/blob/main/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/processor/AbstractProcessor.java) metrics. | Metric name | Type | Description | ------------- | ---- | -----------| `recordsIn` | Counter | Metric representing the ingress of records to a pipeline component. | `recordsOut` | Counter | Metric representing the egress of records from a pipeline component. | `timeElapsed` | Timer | Metric representing the time elapsed during execution of a pipeline component. | The `service-map-stateful` processor includes following custom metrics: * `traceGroupCacheCount`: The number of trace groups in the trace group cache. * `spanSetCount`: The number of span sets in the span set collection. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/service-map-stateful/",
    "relUrl": "/pipelines/configuration/processors/service-map-stateful/"
  },"50": {
    "doc": "Sinks",
    "title": "Sinks",
    "content": "# Sinks Sinks define where Data Prepper writes your data to. ## General options for all sink types The following table describes options you can use to configure the `sinks` sink. Option | Required | Type | Description :--- | :--- | :--- | :--- routes | No | List | List of routes that the sink accepts. If not specified, the sink accepts all upstream events. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sinks/sinks/",
    "relUrl": "/pipelines/configuration/sinks/sinks/"
  },"51": {
    "doc": "Sources",
    "title": "Sources",
    "content": "# Sources Sources define where your data comes from within a Data Prepper pipeline. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sources/sources/",
    "relUrl": "/pipelines/configuration/sources/sources/"
  },"52": {
    "doc": "split_string",
    "title": "split_string",
    "content": "# split_string The `split_string` processor splits a field into an array using a delimiting character and is a [mutate string](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/mutate-string-processors#mutate-string-processors) processor. The following table describes the options you can use to configure the `split_string` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- entries | Yes | List | List of entries. Valid values are `source`, `delimiter`, and `delimiter_regex`. source | N/A | N/A | The key to split. delimiter | No | N/A | The separator character responsible for the split. Cannot be defined at the same time as `delimiter_regex`. At least `delimiter` or `delimiter_regex` must be defined. delimiter_regex | No | N/A | The regex string responsible for the split. Cannot be defined at the same time as `delimiter`. At least `delimiter` or `delimiter_regex` must be defined. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/split-string/",
    "relUrl": "/pipelines/configuration/processors/split-string/"
  },"53": {
    "doc": "stdout sink",
    "title": "stdout sink",
    "content": "# stdout sink ## Overview You can use the `stdout` sink for console output and testing. It has no configurable options. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/sinks/stdout/",
    "relUrl": "/pipelines/configuration/sinks/stdout/"
  },"54": {
    "doc": "string_converter",
    "title": "string_converter",
    "content": "# string_converter The `string_converter` processor converts a string to uppercase or lowercase. You can use it as an example for developing your own processor. The following table describes the option you can use to configure the `string_converter` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- upper_case | No | Boolean | Whether to convert to uppercase (`true`) or lowercase (`false`). ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/string-converter/",
    "relUrl": "/pipelines/configuration/processors/string-converter/"
  },"55": {
    "doc": "substitute_string",
    "title": "substitute_string",
    "content": "# substitute_string The `substitute_string` processor matches a key's value against a regular expression and replaces all matches with a replacement string. `substitute_string` is a [mutate string](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/mutate-string-processors#mutate-string-processors) processor. ## Configuration The following table describes the options you can use to configure the `substitue_string` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- entries | Yes | List | List of entries. Valid values are `source`, `from`, and `to`. source | N/A | N/A | The key to modify. from | N/A | N/A | The Regex String to be replaced. Special regex characters such as `[` and `]` must be escaped using `\\\\` when using double quotes and `\\ ` when using single quotes. See [Java Patterns](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/regex/Pattern.html) for more information. to | N/A | N/A | The String to be substituted for each match of `from`. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/substitute-string/",
    "relUrl": "/pipelines/configuration/processors/substitute-string/"
  },"56": {
    "doc": "Trace analytics",
    "title": "Trace analytics",
    "content": "# Trace analytics Trace analytics allows you to collect trace data and customize a pipeline that ingests and transforms the data for use in OpenSearch. The following provides an overview of the trace analytics workflow in Data Prepper, how to configure it, and how to visualize trace data. ## Introduction When using Data Prepper as a server-side component to collect trace data, you can customize a Data Prepper pipeline to ingest and transform the data for use in OpenSearch. Upon transformation, you can visualize the transformed trace data for use with the Observability plugin inside of OpenSearch Dashboards. Trace data provides visibility into your application's performance, and helps you gain more information about individual traces. The following flowchart illustrates the trace analytics workflow, from running OpenTelemetry Collector to using OpenSearch Dashboards for visualization. {: .img-fluid} To monitor trace analytics, you need to set up the following components in your service environment: - Add **instrumentation** to your application so it can generate telemetry data and send it to an OpenTelemetry collector. - Run an **OpenTelemetry collector** as a sidecar or daemonset for Amazon Elastic Kubernetes Service (Amazon EKS), a sidecar for Amazon Elastic Container Service (Amazon ECS), or an agent on Amazon Elastic Compute Cloud (Amazon EC2). You should configure the collector to export trace data to Data Prepper. - Deploy **Data Prepper** as the ingestion collector for OpenSearch. Configure it to send the enriched trace data to your OpenSearch cluster or to the Amazon OpenSearch Service domain. - Use **OpenSearch Dashboards** to visualize and detect problems in your distributed applications. ## Trace analytics pipeline To monitor trace analytics in Data Prepper, we provide three pipelines: `entry-pipeline`, `raw-trace-pipeline`, and `service-map-pipeline`. The following image provides an overview of how the pipelines work together to monitor trace analytics. {: .img-fluid} ### OpenTelemetry trace source The [OpenTelemetry source]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/otel-trace-raw/) accepts trace data from the OpenTelemetry Collector. The source follows the [OpenTelemetry Protocol](https://github.com/open-telemetry/opentelemetry-specification/tree/master/specification/protocol) and officially supports transport over gRPC and the use of industry-standard encryption (TLS/HTTPS). ### Processor There are three processors for the trace analytics feature: * *otel_trace_raw* - The *otel_trace_raw* processor receives a collection of [span](https://github.com/opensearch-project/data-prepper/blob/fa65e9efb3f8d6a404a1ab1875f21ce85e5c5a6d/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/trace/Span.java) records from [*otel-trace-source*]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sources/otel-trace/), and performs stateful processing, extraction, and completion of trace-group-related fields. * *otel_trace_group* - The *otel_trace_group* processor fills in the missing trace-group-related fields in the collection of [span](https://github.com/opensearch-project/data-prepper/blob/298e7931aa3b26130048ac3bde260e066857df54/data-prepper-api/src/main/java/org/opensearch/dataprepper/model/trace/Span.java) records by looking up the OpenSearch backend. * *service_map_stateful*  The *service_map_stateful* processor performs the required preprocessing for trace data and builds metadata to display the `service-map` dashboards. ### OpenSearch sink OpenSearch provides a generic sink that writes data to OpenSearch as the destination. The [OpenSearch sink]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sinks/opensearch/) has configuration options related to the OpenSearch cluster, such as endpoint, SSL, username/password, index name, index template, and index state management. The sink provides specific configurations for the trace analytics feature. These configurations allow the sink to use indexes and index templates specific to trace analytics. The following OpenSearch indexes are specific to trace analytics: * *otel-v1-apm-span*  The *otel-v1-apm-span* index stores the output from the [otel_trace_raw]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/otel-trace-raw/) processor. * *otel-v1-apm-service-map*  The *otel-v1-apm-service-map* index stores the output from the [service_map_stateful]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/processors/service-map-stateful/) processor. ## Trace tuning Starting with version 0.8.x, Data Prepper supports both vertical and horizontal scaling for trace analytics. You can adjust the size of a single Data Prepper instance to meet your workload's demands and scale vertically. You can scale horizontally by using the core [peer forwarder]({{site.url}}{{site.baseurl}}/data-prepper/managing-data-prepper/peer-forwarder/) to deploy multiple Data Prepper instances to form a cluster. This enables Data Prepper instances to communicate with instances in the cluster and is required for horizontally scaling deployments. ### Scaling recommendations Use the following recommended configurations to scale Data Prepper. We recommend that you modify parameters based on the requirements. We also recommend that you monitor the Data Prepper host metrics and OpenSearch metrics to ensure that the configuration works as expected. #### Buffer The total number of trace requests processed by Data Prepper is equal to the sum of the `buffer_size` values in `otel-trace-pipeline` and `raw-pipeline`. The total number of trace requests sent to OpenSearch is equal to the product of `batch_size` and `workers` in `raw-trace-pipeline`. For more information about `raw-pipeline`, see [Trace analytics pipeline]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/pipelines). We recommend the following when making changes to buffer settings: * The `buffer_size` value in `otel-trace-pipeline` and `raw-pipeline` should be the same. * The `buffer_size` should be greater than or equal to `workers` * `batch_size` in the `raw-pipeline`. #### Workers The `workers` setting determines the number of threads that are used by Data Prepper to process requests from the buffer. We recommend that you set `workers` based on the CPU utilization. This value can be higher than the number of available processors because Data Prepper uses significant input/output time when sending data to OpenSearch. #### Heap Configure the Data Prepper heap by setting the `JVM_OPTS` environment variable. We recommend that you set the heap value to a minimum value of `4` * `batch_size` * `otel_send_batch_size` * `maximum size of indvidual span`. As mentioned in the [OpenTelemetry Collector](#opentelemetry-collector) section, set `otel_send_batch_size` to a value of `50` in your OpenTelemetry Collector configuration. #### Local disk Data Prepper uses the local disk to store metadata required for service map processing, so we recommend storing only the following key fields: `traceId`, `spanId`, `parentSpanId`, `spanKind`, `spanName`, and `serviceName`. The `service-map` plugin stores only two files, each of which stores `window_duration` seconds of data. As an example, testing with a throughput of `3000 spans/second` resulted in the total disk usage of `4 MB`. Data Prepper also uses the local disk to write logs. In the most recent version of Data Prepper, you can redirect the logs to your preferred path. ### AWS CloudFormation template and Kubernetes/Amazon EKS configuration files The [AWS CloudFormation](https://github.com/opensearch-project/data-prepper/blob/main/deployment-template/ec2/data-prepper-ec2-deployment-cfn.yaml) template provides a user-friendly mechanism for configuring the scaling attributes described in the [Trace tuning](#trace-tuning) section. The [Kubernetes configuration files](https://github.com/opensearch-project/data-prepper/blob/main/examples/dev/k8s/README.md) and [Amazon EKS configuration files](https://github.com/opensearch-project/data-prepper/blob/main/deployment-template/eks/README.md) are available for configuring these attributes in a cluster deployment. ### Benchmark tests The benchmark tests were performed on an `r5.xlarge` EC2 instance with the following configuration: * `buffer_size`: 4096 * `batch_size`: 256 * `workers`: 8 * `Heap`: 10 GB This setup was able to handle a throughput of `2100` spans/second at `20` percent CPU utilization. ## Pipeline configuration The following sections provide examples of different types of pipelines and how to configure each type. ### Example: Trace analytics pipeline The following example demonstrates how to build a pipeline that supports the [OpenSearch Dashboards Observability plugin]({{site.url}}{{site.baseurl}}/observability-plugin/trace/ta-dashboards/). This pipeline takes data from the OpenTelemetry Collector and uses two other pipelines as sinks. These two separate pipelines serve two different purposes and write to different OpenSearch indexes. The first pipeline prepares trace data for OpenSearch and enriches and ingests the span documents into a span index within OpenSearch. The second pipeline aggregates traces into a service map and writes service map documents into a service map index within OpenSearch. Starting with Data Prepper version 2.0, Data Prepper no longer supports the `otel_trace_raw_prepper` processor. The `otel_trace_raw` processor replaces the `otel_trace_raw_prepper` processor and supports some of Data Prepper's recent data model changes. Instead, you should use the `otel_trace_raw` processor. See the following YAML file example: ```yml entry-pipeline: delay: \"100\" source: otel_trace_source: ssl: false buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 sink: - pipeline: name: \"raw-trace-pipeline\" - pipeline: name: \"service-map-pipeline\" raw-pipeline: source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - otel_trace_raw: sink: - opensearch: hosts: [\"https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-raw service-map-pipeline: delay: \"100\" source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - service_map_stateful: sink: - opensearch: hosts: [\"https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-service-map ``` To maintain similar ingestion throughput and latency, scale the `buffer_size` and `batch_size` by the estimated maximum batch size in the client request payload. {: .tip} #### Example: `otel trace` The following is an example `otel-trace-source` .yaml file with SSL and basic authentication enabled. Note that you will need to modify your `otel-collector-config.yaml` file so that it uses your own credentials. ```yaml source: otel_trace_source: #record_type: event # Add this when using Data Prepper 1.x. This option is removed in 2.0 ssl: true sslKeyCertChainFile: \"/full/path/to/certfile.crt\" sslKeyFile: \"/full/path/to/keyfile.key\" authentication: http_basic: username: \"my-user\" password: \"my_s3cr3t\" ``` #### Example: pipeline.yaml The following is an example `pipeline.yaml` file without SSL and basic authentication enabled for the `otel-trace-pipeline` pipeline: ```yaml otel-trace-pipeline: # workers is the number of threads processing data in each pipeline. # We recommend same value for all pipelines. # default value is 1, set a value based on the machine you are running Data Prepper workers: 8 # delay in milliseconds is how often the worker threads should process data. # Recommend not to change this config as we want the entry-pipeline to process as quick as possible # default value is 3_000 ms delay: \"100\" source: otel_trace_source: #record_type: event # Add this when using Data Prepper 1.x. This option is removed in 2.0 ssl: false # Change this to enable encryption in transit authentication: unauthenticated: buffer: bounded_blocking: # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. # We recommend to keep the same buffer_size for all pipelines. # Make sure you configure sufficient heap # default value is 512 buffer_size: 512 # This is the maximum number of request each worker thread will process within the delay. # Default is 8. # Make sure buffer_size >= workers * batch_size batch_size: 8 sink: - pipeline: name: \"raw-trace-pipeline\" - pipeline: name: \"entry-pipeline\" raw-pipeline: # Configure same as the otel-trace-pipeline workers: 8 # We recommend using the default value for the raw-pipeline. delay: \"3000\" source: pipeline: name: \"entry-pipeline\" buffer: bounded_blocking: # Configure the same value as in entry-pipeline # Make sure you configure sufficient heap # The default value is 512 buffer_size: 512 # The raw processor does bulk request to your OpenSearch sink, so configure the batch_size higher. # If you use the recommended otel-collector setup each ExportTraceRequest could contain max 50 spans. https://github.com/opensearch-project/data-prepper/tree/v0.7.x/deployment/aws # With 64 as batch size each worker thread could process upto 3200 spans (64 * 50) batch_size: 64 processor: - otel_trace_raw: - otel_trace_group: hosts: [ \"https://localhost:9200\" ] # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 sink: - opensearch: hosts: [ \"https://localhost:9200\" ] index_type: trace-analytics-raw # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 service-map-pipeline: workers: 8 delay: \"100\" source: pipeline: name: \"entry-pipeline\" processor: - service_map_stateful: # The window duration is the maximum length of time the data prepper stores the most recent trace data to evaluvate service-map relationships. # The default is 3 minutes, this means we can detect relationships between services from spans reported in last 3 minutes. # Set higher value if your applications have higher latency. window_duration: 180 buffer: bounded_blocking: # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. # We recommend to keep the same buffer_size for all pipelines. # Make sure you configure sufficient heap # default value is 512 buffer_size: 512 # This is the maximum number of request each worker thread will process within the delay. # Default is 8. # Make sure buffer_size >= workers * batch_size batch_size: 8 sink: - opensearch: hosts: [ \"https://localhost:9200\" ] index_type: trace-analytics-service-map # Change to your credentials username: \"admin\" password: \"admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 ``` You need to modify the preceding configuration for your OpenSearch cluster so that the configuration matches your environment. Note that it has two `opensearch` sinks that need to be modified. {: .note} You must make the following changes: * `hosts`  Set to your hosts. * `username`  Provide your OpenSearch username. * `password`  Provide your OpenSearch password. * `aws_sigv4`  If you are using Amazon OpenSearch Service with AWS signing, set this value to `true`. It will sign requests with the default AWS credentials provider. * `aws_region`  If you are using Amazon OpenSearch Service with AWS signing, set this value to your AWS Region. For other configurations available for OpenSearch sinks, see [Data Prepper OpenSearch sink]({{site.url}}{{site.baseurl}}/data-prepper/pipelines/configuration/sinks/opensearch/). ## OpenTelemetry Collector You need to run OpenTelemetry Collector in your service environment. Follow [Getting Started](https://opentelemetry.io/docs/collector/getting-started/#getting-started) to install an OpenTelemetry collector. Ensure that you configure the collector with an exporter configured for your Data Prepper instance. The following example `otel-collector-config.yaml` file receives data from various instrumentations and exports it to Data Prepper. ### Example otel-collector-config.yaml file The following is an example `otel-collector-config.yaml` file: ``` receivers: jaeger: protocols: grpc: otlp: protocols: grpc: zipkin: processors: batch/traces: timeout: 1s send_batch_size: 50 exporters: otlp/data-prepper: endpoint: localhost:21890 tls: insecure: true service: pipelines: traces: receivers: [jaeger, otlp, zipkin] processors: [batch/traces] exporters: [otlp/data-prepper] ``` After you run OpenTelemetry in your service environment, you must configure your application to use the OpenTelemetry Collector. The OpenTelemetry Collector typically runs alongside your application. ## Next steps and more information The [OpenSearch Dashboards Observability plugin]({{site.url}}{{site.baseurl}}/observability-plugin/trace/ta-dashboards/) documentation provides additional information about configuring OpenSearch to view trace analytics in OpenSearch Dashboards. For more information about how to tune and scale Data Prepper for trace analytics, see [Trace tuning](#trace-tuning). ## Migrating to Data Prepper 2.0 Starting with Data Prepper version 1.4, trace processing uses Data Prepper's event model. This allows pipeline authors to configure other processors to modify spans or traces. To provide a migration path, Data Prepper version 1.4 introduced the following changes: * `otel_trace_source` has an optional `record_type` parameter that can be set to `event`. When configured, it will output event objects. * `otel_trace_raw` replaces `otel_trace_raw_prepper` for event-based spans. * `otel_trace_group` replaces `otel_trace_group_prepper` for event-based spans. In Data Prepper version 2.0, `otel_trace_source` will only output events. Data Prepper version 2.0 also removes `otel_trace_raw_prepper` and `otel_trace_group_prepper` entirely. To migrate to Data Prepper version 2.0, you can configure your trace pipeline using the event model. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/common-use-cases/trace-analytics/",
    "relUrl": "/common-use-cases/trace-analytics/"
  },"57": {
    "doc": "trace_peer_forwarder",
    "title": "trace_peer_forwarder",
    "content": "# trace peer forwarder The `trace_peer_forwarder` processor is used with [peer forwarder]({{site.url}}{{site.baseurl}}/data-prepper/managing-data-prepper/peer-forwarder/) to reduce by half the number of events forwarded in a [Trace Analytics]({{site.url}}{{site.baseurl}}/data-prepper/common-use-cases/trace-analytics/) pipeline. In Trace Analytics, each event is typically duplicated when it is sent from `otel-trace-pipeline` to `raw-pipeline` and `service-map-pipeline`. When pipelines forward events, this causes the core peer forwarder to send multiple HTTP requests for the same event. You can use `trace peer forwarder` to forward an event once through the `otel-trace-pipeline` instead of `raw-pipeline` and `service-map-pipeline`, which prevents unnecessary HTTP requests. You should use `trace_peer_forwarder` for Trace Analytics pipelines when you have multiple nodes. ## Usage To get started with `trace_peer_forwarder`, first configure [peer forwarder]({{site.url}}{{site.baseurl}}/data-prepper/managing-data-prepper/peer-forwarder/). Then create a `pipeline.yaml` file and specify `trace peer forwarder` as the processor. You can configure `peer forwarder` in your `data-prepper-config.yaml` file. For more detailed information, see [Configuring Data Prepper]({{site.url}}{{site.baseurl}}/data-prepper/getting-started/#2-configuring-data-prepper). See the following example `pipeline.yaml` file: ```yaml otel-trace-pipeline: delay: \"100\" source: otel_trace_source: processor: - trace_peer_forwarder: sink: - pipeline: name: \"raw-pipeline\" - pipeline: name: \"service-map-pipeline\" raw-pipeline: source: pipeline: name: \"entry-pipeline\" processor: - otel_trace_raw: sink: - opensearch: service-map-pipeline: delay: \"100\" source: pipeline: name: \"entry-pipeline\" processor: - service_map_stateful: sink: - opensearch: ``` In the preceding `pipeline.yaml` file, events are forwarded in the `otel-trace-pipeline` to the target peer, and no forwarding is performed in `raw-pipeline` or `service-map-pipeline`. This process helps improve network performance by forwarding events (as HTTP requests) once instead of twice. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/trace-peer-forwarder/",
    "relUrl": "/pipelines/configuration/processors/trace-peer-forwarder/"
  },"58": {
    "doc": "trim_string",
    "title": "trim_string",
    "content": "# trim_string The `trim_string` processor removes whitespace from the beginning and end of a key and is a [mutate string](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/mutate-string-processors#mutate-string-processors) processor. The following table describes the option you can use to configure the `trim_string` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- with_keys | Yes | List | A list of keys to trim the whitespace from. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/trim-string/",
    "relUrl": "/pipelines/configuration/processors/trim-string/"
  },"59": {
    "doc": "uppercase_string",
    "title": "uppercase_string",
    "content": "# uppercase_string The `uppercase_string` processor converts an entire string to uppercase and is a [mutate string](https://github.com/opensearch-project/data-prepper/tree/main/data-prepper-plugins/mutate-string-processors#mutate-string-processors) processor. The following table describes the option you can use to configure the `uppercase_string` processor. Option | Required | Type | Description :--- | :--- | :--- | :--- with_keys | Yes | List | A list of keys to convert to uppercase. ",
    "url": "http://localhost:4000/data-prepper/docs/latest/pipelines/configuration/processors/uppercase-string/",
    "relUrl": "/pipelines/configuration/processors/uppercase-string/"
  }
}
